{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3070aa7e",
   "metadata": {},
   "source": [
    "# OpenAI API Error Mitigation Strategies\n",
    "\n",
    "This notebook demonstrates comprehensive error mitigation strategies for production-ready OpenAI API integration, including exponential backoff, rate limiting, quota management, and robust error handling patterns.\n",
    "\n",
    "## 🎯 Key Objectives\n",
    "\n",
    "- **Prevent Rate Limit Errors**: Implement smart retry logic with exponential backoff\n",
    "- **Monitor Usage**: Track API consumption across multiple dimensions\n",
    "- **Enforce Quotas**: Implement user-level usage limits and controls\n",
    "- **Handle Failures Gracefully**: Build resilient systems that degrade gracefully\n",
    "- **Optimize Costs**: Balance performance, reliability, and API costs\n",
    "\n",
    "## 📊 Error Mitigation Benefits\n",
    "\n",
    "1. **Automatic Recovery** - Retry failed requests without data loss\n",
    "2. **Cost Optimization** - Avoid unnecessary API calls and overages\n",
    "3. **User Experience** - Maintain service availability during high load\n",
    "4. **Monitoring & Alerts** - Proactive notification of quota/limit issues\n",
    "5. **Scalability** - Handle varying load patterns effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cffaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Callable, Any\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict, deque\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "\n",
    "# Configure logging for error monitoring\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('api_errors.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger('OpenAI_ErrorMitigation')\n",
    "\n",
    "print(\"✅ All required libraries imported successfully!\")\n",
    "print(\"📝 Logging configured for error monitoring\")\n",
    "print(\"🔧 Ready to implement error mitigation strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266fcf07",
   "metadata": {},
   "source": [
    "## 🔄 Basic Exponential Backoff Implementation\n",
    "\n",
    "Exponential backoff is a fundamental error recovery technique that increases wait times between retry attempts. This prevents overwhelming the API server and allows temporary issues to resolve.\n",
    "\n",
    "### Key Features:\n",
    "- **Exponential Growth**: Each retry waits 2x longer than the previous attempt\n",
    "- **Maximum Backoff**: Prevents extremely long waits\n",
    "- **Retry Limits**: Avoids infinite retry loops\n",
    "- **Error Classification**: Only retries recoverable errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b633a884",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialBackoff:\n",
    "    \"\"\"\n",
    "    Basic exponential backoff implementation for API retry logic\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_delay=1.0, max_delay=60.0, max_retries=5, backoff_factor=2.0):\n",
    "        self.base_delay = base_delay\n",
    "        self.max_delay = max_delay\n",
    "        self.max_retries = max_retries\n",
    "        self.backoff_factor = backoff_factor\n",
    "        \n",
    "    def get_delay(self, attempt: int) -> float:\n",
    "        \"\"\"Calculate delay for given attempt number\"\"\"\n",
    "        if attempt == 0:\n",
    "            return 0\n",
    "        \n",
    "        delay = self.base_delay * (self.backoff_factor ** (attempt - 1))\n",
    "        return min(delay, self.max_delay)\n",
    "    \n",
    "    def should_retry(self, error: Exception, attempt: int) -> bool:\n",
    "        \"\"\"Determine if we should retry based on error type and attempt count\"\"\"\n",
    "        if attempt >= self.max_retries:\n",
    "            return False\n",
    "            \n",
    "        # Define retryable errors\n",
    "        retryable_errors = [\n",
    "            'rate_limit_exceeded',\n",
    "            'server_error',\n",
    "            'timeout',\n",
    "            'connection_error',\n",
    "            '429',  # Too Many Requests\n",
    "            '500',  # Internal Server Error\n",
    "            '502',  # Bad Gateway\n",
    "            '503',  # Service Unavailable\n",
    "            '504'   # Gateway Timeout\n",
    "        ]\n",
    "        \n",
    "        error_message = str(error).lower()\n",
    "        return any(retryable in error_message for retryable in retryable_errors)\n",
    "    \n",
    "    async def execute_with_backoff(self, func: Callable, *args, **kwargs) -> Any:\n",
    "        \"\"\"Execute function with exponential backoff retry logic\"\"\"\n",
    "        last_error = None\n",
    "        \n",
    "        for attempt in range(self.max_retries + 1):\n",
    "            try:\n",
    "                logger.info(f\"Attempting request (attempt {attempt + 1}/{self.max_retries + 1})\")\n",
    "                \n",
    "                # Execute the function\n",
    "                result = await func(*args, **kwargs) if asyncio.iscoroutinefunction(func) else func(*args, **kwargs)\n",
    "                \n",
    "                if attempt > 0:\n",
    "                    logger.info(f\"✅ Request succeeded on attempt {attempt + 1}\")\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except Exception as error:\n",
    "                last_error = error\n",
    "                logger.warning(f\"❌ Request failed on attempt {attempt + 1}: {error}\")\n",
    "                \n",
    "                if not self.should_retry(error, attempt):\n",
    "                    logger.error(f\"🚫 Not retrying: {'Max retries exceeded' if attempt >= self.max_retries else 'Non-retryable error'}\")\n",
    "                    break\n",
    "                \n",
    "                delay = self.get_delay(attempt + 1)\n",
    "                if delay > 0:\n",
    "                    logger.info(f\"⏱️ Waiting {delay:.2f}s before retry...\")\n",
    "                    await asyncio.sleep(delay) if asyncio.iscoroutinefunction(func) else time.sleep(delay)\n",
    "        \n",
    "        raise last_error\n",
    "\n",
    "# Test the basic exponential backoff\n",
    "backoff = ExponentialBackoff(base_delay=0.5, max_delay=10.0, max_retries=4)\n",
    "\n",
    "print(\"📊 Exponential Backoff Delays:\")\n",
    "for i in range(6):\n",
    "    delay = backoff.get_delay(i)\n",
    "    print(f\"  Attempt {i}: {delay:.2f}s delay\")\n",
    "\n",
    "print(\"\\n✅ Basic Exponential Backoff implementation ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a664e6a8",
   "metadata": {},
   "source": [
    "## 🎲 Advanced Retry Logic with Jitter\n",
    "\n",
    "Adding randomized jitter to exponential backoff prevents the \"thundering herd\" problem where multiple clients retry simultaneously, potentially overwhelming the server again.\n",
    "\n",
    "### Jitter Benefits:\n",
    "- **Spreads Load**: Randomizes retry timing across clients\n",
    "- **Reduces Collisions**: Prevents synchronized retry storms\n",
    "- **Improves Success Rate**: Increases chance of individual request success\n",
    "- **Scales Better**: More effective with multiple concurrent clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8316e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JitteredBackoff(ExponentialBackoff):\n",
    "    \"\"\"\n",
    "    Advanced exponential backoff with randomized jitter\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, jitter_factor=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.jitter_factor = jitter_factor  # 0.1 = 10% jitter\n",
    "        \n",
    "    def get_delay(self, attempt: int) -> float:\n",
    "        \"\"\"Calculate delay with random jitter\"\"\"\n",
    "        base_delay = super().get_delay(attempt)\n",
    "        \n",
    "        if base_delay == 0:\n",
    "            return 0\n",
    "            \n",
    "        # Add random jitter: ±jitter_factor of the base delay\n",
    "        jitter_range = base_delay * self.jitter_factor\n",
    "        jitter = random.uniform(-jitter_range, jitter_range)\n",
    "        \n",
    "        final_delay = max(0.1, base_delay + jitter)  # Minimum 0.1s delay\n",
    "        return min(final_delay, self.max_delay)\n",
    "\n",
    "class SmartRetryHandler:\n",
    "    \"\"\"\n",
    "    Intelligent retry handler with multiple strategies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_delay=1.0, \n",
    "                 max_delay=60.0, \n",
    "                 max_retries=5, \n",
    "                 jitter_factor=0.1,\n",
    "                 rate_limit_window=60):\n",
    "        \n",
    "        self.backoff = JitteredBackoff(\n",
    "            base_delay=base_delay,\n",
    "            max_delay=max_delay,\n",
    "            max_retries=max_retries,\n",
    "            jitter_factor=jitter_factor\n",
    "        )\n",
    "        \n",
    "        # Track rate limiting\n",
    "        self.request_times = deque()\n",
    "        self.rate_limit_window = rate_limit_window\n",
    "        self.rate_limit_hit_count = 0\n",
    "        \n",
    "    def is_rate_limited(self, requests_per_window=60) -> bool:\n",
    "        \"\"\"Check if we're hitting rate limits\"\"\"\n",
    "        now = time.time()\n",
    "        \n",
    "        # Remove old requests outside the window\n",
    "        while self.request_times and (now - self.request_times[0]) > self.rate_limit_window:\n",
    "            self.request_times.popleft()\n",
    "            \n",
    "        return len(self.request_times) >= requests_per_window\n",
    "        \n",
    "    def record_request(self):\n",
    "        \"\"\"Record a request timestamp\"\"\"\n",
    "        self.request_times.append(time.time())\n",
    "        \n",
    "    def get_adaptive_delay(self, attempt: int, error: Exception) -> float:\n",
    "        \"\"\"Get adaptive delay based on error type and rate limit status\"\"\"\n",
    "        base_delay = self.backoff.get_delay(attempt)\n",
    "        \n",
    "        error_message = str(error).lower()\n",
    "        \n",
    "        # Longer delays for quota/rate limit errors\n",
    "        if 'rate_limit' in error_message or 'quota' in error_message or '429' in error_message:\n",
    "            self.rate_limit_hit_count += 1\n",
    "            # Exponentially increase delay for repeated rate limits\n",
    "            rate_limit_multiplier = min(4.0, 1.5 ** self.rate_limit_hit_count)\n",
    "            base_delay *= rate_limit_multiplier\n",
    "            logger.warning(f\"🚦 Rate limit detected, applying {rate_limit_multiplier:.1f}x multiplier\")\n",
    "            \n",
    "        # Shorter delays for server errors (they often resolve quickly)\n",
    "        elif any(code in error_message for code in ['500', '502', '503', '504']):\n",
    "            base_delay *= 0.7\n",
    "            \n",
    "        return min(base_delay, self.backoff.max_delay)\n",
    "\n",
    "# Test jittered backoff with multiple samples\n",
    "jittered_backoff = JitteredBackoff(base_delay=1.0, jitter_factor=0.2, max_retries=5)\n",
    "\n",
    "print(\"📊 Jittered Backoff Delays (5 samples per attempt):\")\n",
    "for attempt in range(4):\n",
    "    delays = [jittered_backoff.get_delay(attempt) for _ in range(5)]\n",
    "    avg_delay = sum(delays) / len(delays)\n",
    "    min_delay = min(delays)\n",
    "    max_delay = max(delays)\n",
    "    print(f\"  Attempt {attempt}: {avg_delay:.2f}s avg ({min_delay:.2f}s - {max_delay:.2f}s)\")\n",
    "\n",
    "# Test smart retry handler\n",
    "smart_handler = SmartRetryHandler(jitter_factor=0.15)\n",
    "\n",
    "print(f\"\\n✅ Advanced Retry Logic with Jitter implemented!\")\n",
    "print(f\"🎲 Jitter factor: {jittered_backoff.jitter_factor * 100}%\")\n",
    "print(f\"📊 Rate limiting window: {smart_handler.rate_limit_window}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5174dc6f",
   "metadata": {},
   "source": [
    "## 📊 Rate Limit Monitoring and Tracking\n",
    "\n",
    "Comprehensive rate limit monitoring prevents hitting API limits before they occur. This proactive approach maintains service availability and optimizes API usage costs.\n",
    "\n",
    "### Monitoring Dimensions:\n",
    "- **Requests per Minute/Hour**: Track request frequency\n",
    "- **Tokens per Minute**: Monitor token consumption rates  \n",
    "- **Concurrent Requests**: Limit simultaneous API calls\n",
    "- **User-Level Quotas**: Individual user consumption tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c341873",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RateLimitConfig:\n",
    "    \"\"\"Configuration for rate limiting\"\"\"\n",
    "    requests_per_minute: int = 60\n",
    "    tokens_per_minute: int = 40000\n",
    "    requests_per_hour: int = 3000\n",
    "    tokens_per_hour: int = 2000000\n",
    "    max_concurrent: int = 10\n",
    "    burst_allowance: float = 1.2  # 20% burst capacity\n",
    "\n",
    "@dataclass\n",
    "class UsageMetrics:\n",
    "    \"\"\"Track API usage metrics\"\"\"\n",
    "    requests_count: int = 0\n",
    "    tokens_used: int = 0\n",
    "    errors_count: int = 0\n",
    "    last_request_time: float = 0\n",
    "    requests_this_minute: List[float] = field(default_factory=list)\n",
    "    tokens_this_minute: List[int] = field(default_factory=list)\n",
    "\n",
    "class RateLimitMonitor:\n",
    "    \"\"\"\n",
    "    Comprehensive rate limit monitoring and enforcement\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: RateLimitConfig):\n",
    "        self.config = config\n",
    "        self.metrics = UsageMetrics()\n",
    "        self.user_metrics: Dict[str, UsageMetrics] = defaultdict(lambda: UsageMetrics())\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "    def cleanup_old_entries(self):\n",
    "        \"\"\"Remove entries older than 1 minute\"\"\"\n",
    "        cutoff_time = time.time() - 60\n",
    "        \n",
    "        # Clean global metrics\n",
    "        self.metrics.requests_this_minute = [\n",
    "            t for t in self.metrics.requests_this_minute if t > cutoff_time\n",
    "        ]\n",
    "        self.metrics.tokens_this_minute = self.metrics.tokens_this_minute[-len(self.metrics.requests_this_minute):]\n",
    "        \n",
    "        # Clean user metrics\n",
    "        for user_id, metrics in self.user_metrics.items():\n",
    "            metrics.requests_this_minute = [\n",
    "                t for t in metrics.requests_this_minute if t > cutoff_time\n",
    "            ]\n",
    "            metrics.tokens_this_minute = metrics.tokens_this_minute[-len(metrics.requests_this_minute):]\n",
    "    \n",
    "    def can_make_request(self, user_id: str = None, estimated_tokens: int = 1000) -> Dict[str, Any]:\n",
    "        \"\"\"Check if a request can be made based on current limits\"\"\"\n",
    "        with self.lock:\n",
    "            self.cleanup_old_entries()\n",
    "            \n",
    "            # Check global limits\n",
    "            current_rpm = len(self.metrics.requests_this_minute)\n",
    "            current_tpm = sum(self.metrics.tokens_this_minute)\n",
    "            \n",
    "            global_rpm_ok = current_rpm < (self.config.requests_per_minute * self.config.burst_allowance)\n",
    "            global_tpm_ok = (current_tpm + estimated_tokens) < (self.config.tokens_per_minute * self.config.burst_allowance)\n",
    "            \n",
    "            result = {\n",
    "                'allowed': global_rpm_ok and global_tpm_ok,\n",
    "                'global_rpm': current_rpm,\n",
    "                'global_tpm': current_tpm,\n",
    "                'rpm_limit': self.config.requests_per_minute,\n",
    "                'tpm_limit': self.config.tokens_per_minute,\n",
    "                'estimated_tokens': estimated_tokens,\n",
    "                'wait_time': 0\n",
    "            }\n",
    "            \n",
    "            # Check user-specific limits if user_id provided\n",
    "            if user_id:\n",
    "                user_metrics = self.user_metrics[user_id]\n",
    "                user_rpm = len(user_metrics.requests_this_minute)\n",
    "                user_tpm = sum(user_metrics.tokens_this_minute)\n",
    "                \n",
    "                # User limits (typically lower than global)\n",
    "                user_rpm_limit = min(self.config.requests_per_minute // 4, 20)\n",
    "                user_tpm_limit = min(self.config.tokens_per_minute // 4, 10000)\n",
    "                \n",
    "                user_rpm_ok = user_rpm < user_rpm_limit\n",
    "                user_tpm_ok = (user_tpm + estimated_tokens) < user_tpm_limit\n",
    "                \n",
    "                result.update({\n",
    "                    'user_rpm': user_rpm,\n",
    "                    'user_tpm': user_tpm,\n",
    "                    'user_rpm_limit': user_rpm_limit,\n",
    "                    'user_tpm_limit': user_tpm_limit,\n",
    "                    'user_allowed': user_rpm_ok and user_tpm_ok\n",
    "                })\n",
    "                \n",
    "                result['allowed'] = result['allowed'] and result['user_allowed']\n",
    "            \n",
    "            # Calculate wait time if request not allowed\n",
    "            if not result['allowed']:\n",
    "                # Time until oldest request expires\n",
    "                if self.metrics.requests_this_minute:\n",
    "                    wait_time = 60 - (time.time() - self.metrics.requests_this_minute[0]) + 1\n",
    "                    result['wait_time'] = max(0, wait_time)\n",
    "                \n",
    "            return result\n",
    "    \n",
    "    def record_request(self, user_id: str = None, tokens_used: int = 0, success: bool = True):\n",
    "        \"\"\"Record a completed request\"\"\"\n",
    "        with self.lock:\n",
    "            current_time = time.time()\n",
    "            \n",
    "            # Update global metrics\n",
    "            self.metrics.requests_count += 1\n",
    "            self.metrics.tokens_used += tokens_used\n",
    "            self.metrics.last_request_time = current_time\n",
    "            self.metrics.requests_this_minute.append(current_time)\n",
    "            self.metrics.tokens_this_minute.append(tokens_used)\n",
    "            \n",
    "            if not success:\n",
    "                self.metrics.errors_count += 1\n",
    "            \n",
    "            # Update user metrics\n",
    "            if user_id:\n",
    "                user_metrics = self.user_metrics[user_id]\n",
    "                user_metrics.requests_count += 1\n",
    "                user_metrics.tokens_used += tokens_used\n",
    "                user_metrics.last_request_time = current_time\n",
    "                user_metrics.requests_this_minute.append(current_time)\n",
    "                user_metrics.tokens_this_minute.append(tokens_used)\n",
    "                \n",
    "                if not success:\n",
    "                    user_metrics.errors_count += 1\n",
    "    \n",
    "    def get_status(self, user_id: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Get current rate limit status\"\"\"\n",
    "        with self.lock:\n",
    "            self.cleanup_old_entries()\n",
    "            \n",
    "            status = {\n",
    "                'global': {\n",
    "                    'requests_per_minute': len(self.metrics.requests_this_minute),\n",
    "                    'tokens_per_minute': sum(self.metrics.tokens_this_minute),\n",
    "                    'total_requests': self.metrics.requests_count,\n",
    "                    'total_tokens': self.metrics.tokens_used,\n",
    "                    'total_errors': self.metrics.errors_count,\n",
    "                    'rpm_utilization': len(self.metrics.requests_this_minute) / self.config.requests_per_minute,\n",
    "                    'tpm_utilization': sum(self.metrics.tokens_this_minute) / self.config.tokens_per_minute\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            if user_id and user_id in self.user_metrics:\n",
    "                user_metrics = self.user_metrics[user_id]\n",
    "                status['user'] = {\n",
    "                    'requests_per_minute': len(user_metrics.requests_this_minute),\n",
    "                    'tokens_per_minute': sum(user_metrics.tokens_this_minute),\n",
    "                    'total_requests': user_metrics.requests_count,\n",
    "                    'total_tokens': user_metrics.tokens_used,\n",
    "                    'total_errors': user_metrics.errors_count\n",
    "                }\n",
    "            \n",
    "            return status\n",
    "\n",
    "# Initialize rate limit monitoring\n",
    "config = RateLimitConfig(\n",
    "    requests_per_minute=20,\n",
    "    tokens_per_minute=30000,\n",
    "    max_concurrent=5\n",
    ")\n",
    "\n",
    "monitor = RateLimitMonitor(config)\n",
    "\n",
    "# Simulate some requests\n",
    "print(\"📊 Simulating API requests...\")\n",
    "for i in range(5):\n",
    "    can_request = monitor.can_make_request(user_id=\"test_user\", estimated_tokens=1000)\n",
    "    print(f\"Request {i+1}: {'✅ Allowed' if can_request['allowed'] else '❌ Blocked'}\")\n",
    "    \n",
    "    if can_request['allowed']:\n",
    "        # Simulate successful request\n",
    "        monitor.record_request(user_id=\"test_user\", tokens_used=random.randint(800, 1200), success=True)\n",
    "        time.sleep(0.1)  # Small delay between requests\n",
    "\n",
    "# Get current status\n",
    "status = monitor.get_status(user_id=\"test_user\")\n",
    "print(f\"\\n📈 Current Usage Status:\")\n",
    "print(f\"Global RPM: {status['global']['requests_per_minute']}/{config.requests_per_minute}\")\n",
    "print(f\"Global TPM: {status['global']['tokens_per_minute']:,}/{config.tokens_per_minute:,}\")\n",
    "print(f\"RPM Utilization: {status['global']['rpm_utilization']:.1%}\")\n",
    "print(f\"TPM Utilization: {status['global']['tpm_utilization']:.1%}\")\n",
    "\n",
    "if 'user' in status:\n",
    "    print(f\"User Requests: {status['user']['requests_per_minute']}\")\n",
    "    print(f\"User Tokens: {status['user']['tokens_per_minute']:,}\")\n",
    "\n",
    "print(\"\\n✅ Rate Limit Monitoring system operational!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34175f3f",
   "metadata": {},
   "source": [
    "## 🎫 Usage Limit Implementation\n",
    "\n",
    "Implement user quota systems with daily, weekly, and monthly limits to prevent abuse and control costs. This includes hard caps, soft warnings, and manual review processes.\n",
    "\n",
    "### Quota Management Features:\n",
    "- **Multi-Timeframe Limits**: Daily, weekly, monthly quotas\n",
    "- **Graduated Response**: Warnings → Throttling → Hard stops\n",
    "- **Trusted User Exceptions**: Higher limits for verified users\n",
    "- **Usage Analytics**: Detailed consumption reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f587ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class UserTier(Enum):\n",
    "    FREE = \"free\"\n",
    "    BASIC = \"basic\"\n",
    "    PREMIUM = \"premium\"\n",
    "    ENTERPRISE = \"enterprise\"\n",
    "\n",
    "class QuotaPeriod(Enum):\n",
    "    DAILY = \"daily\"\n",
    "    WEEKLY = \"weekly\"\n",
    "    MONTHLY = \"monthly\"\n",
    "\n",
    "@dataclass\n",
    "class QuotaLimits:\n",
    "    \"\"\"Quota limits for different user tiers and time periods\"\"\"\n",
    "    requests: int\n",
    "    tokens: int\n",
    "    period: QuotaPeriod\n",
    "    \n",
    "# Define quota limits by user tier\n",
    "QUOTA_LIMITS = {\n",
    "    UserTier.FREE: {\n",
    "        QuotaPeriod.DAILY: QuotaLimits(100, 50000, QuotaPeriod.DAILY),\n",
    "        QuotaPeriod.WEEKLY: QuotaLimits(500, 250000, QuotaPeriod.WEEKLY),\n",
    "        QuotaPeriod.MONTHLY: QuotaLimits(2000, 1000000, QuotaPeriod.MONTHLY)\n",
    "    },\n",
    "    UserTier.BASIC: {\n",
    "        QuotaPeriod.DAILY: QuotaLimits(500, 250000, QuotaPeriod.DAILY),\n",
    "        QuotaPeriod.WEEKLY: QuotaLimits(3000, 1500000, QuotaPeriod.WEEKLY),\n",
    "        QuotaPeriod.MONTHLY: QuotaLimits(12000, 6000000, QuotaPeriod.MONTHLY)\n",
    "    },\n",
    "    UserTier.PREMIUM: {\n",
    "        QuotaPeriod.DAILY: QuotaLimits(2000, 1000000, QuotaPeriod.DAILY),\n",
    "        QuotaPeriod.WEEKLY: QuotaLimits(12000, 6000000, QuotaPeriod.WEEKLY),\n",
    "        QuotaPeriod.MONTHLY: QuotaLimits(50000, 25000000, QuotaPeriod.MONTHLY)\n",
    "    },\n",
    "    UserTier.ENTERPRISE: {\n",
    "        QuotaPeriod.DAILY: QuotaLimits(10000, 5000000, QuotaPeriod.DAILY),\n",
    "        QuotaPeriod.WEEKLY: QuotaLimits(60000, 30000000, QuotaPeriod.WEEKLY),\n",
    "        QuotaPeriod.MONTHLY: QuotaLimits(250000, 125000000, QuotaPeriod.MONTHLY)\n",
    "    }\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class UsageRecord:\n",
    "    \"\"\"Track usage for a specific time period\"\"\"\n",
    "    period_start: datetime\n",
    "    period_end: datetime\n",
    "    requests_used: int = 0\n",
    "    tokens_used: int = 0\n",
    "    last_updated: datetime = field(default_factory=datetime.now)\n",
    "\n",
    "class UsageQuotaManager:\n",
    "    \"\"\"\n",
    "    Comprehensive usage quota management system\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.user_usage: Dict[str, Dict[QuotaPeriod, UsageRecord]] = defaultdict(lambda: {})\n",
    "        self.user_tiers: Dict[str, UserTier] = {}\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "    def set_user_tier(self, user_id: str, tier: UserTier):\n",
    "        \"\"\"Set user tier for quota calculations\"\"\"\n",
    "        with self.lock:\n",
    "            self.user_tiers[user_id] = tier\n",
    "            logger.info(f\"👤 User {user_id} assigned to {tier.value} tier\")\n",
    "    \n",
    "    def get_period_dates(self, period: QuotaPeriod) -> tuple[datetime, datetime]:\n",
    "        \"\"\"Get start and end dates for the current quota period\"\"\"\n",
    "        now = datetime.now()\n",
    "        \n",
    "        if period == QuotaPeriod.DAILY:\n",
    "            start = now.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "            end = start + timedelta(days=1)\n",
    "        elif period == QuotaPeriod.WEEKLY:\n",
    "            # Start of current week (Monday)\n",
    "            days_since_monday = now.weekday()\n",
    "            start = (now - timedelta(days=days_since_monday)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "            end = start + timedelta(weeks=1)\n",
    "        else:  # MONTHLY\n",
    "            start = now.replace(day=1, hour=0, minute=0, second=0, microsecond=0)\n",
    "            if now.month == 12:\n",
    "                end = start.replace(year=now.year + 1, month=1)\n",
    "            else:\n",
    "                end = start.replace(month=now.month + 1)\n",
    "        \n",
    "        return start, end\n",
    "    \n",
    "    def get_or_create_usage_record(self, user_id: str, period: QuotaPeriod) -> UsageRecord:\n",
    "        \"\"\"Get or create usage record for user and period\"\"\"\n",
    "        period_start, period_end = self.get_period_dates(period)\n",
    "        \n",
    "        if period not in self.user_usage[user_id]:\n",
    "            self.user_usage[user_id][period] = UsageRecord(period_start, period_end)\n",
    "        else:\n",
    "            # Check if current record is for the right period\n",
    "            existing = self.user_usage[user_id][period]\n",
    "            if existing.period_start != period_start:\n",
    "                # Create new record for current period\n",
    "                self.user_usage[user_id][period] = UsageRecord(period_start, period_end)\n",
    "        \n",
    "        return self.user_usage[user_id][period]\n",
    "    \n",
    "    def check_quota(self, user_id: str, requested_tokens: int = 1000) -> Dict[str, Any]:\n",
    "        \"\"\"Check if user can make request within quota limits\"\"\"\n",
    "        with self.lock:\n",
    "            user_tier = self.user_tiers.get(user_id, UserTier.FREE)\n",
    "            limits = QUOTA_LIMITS[user_tier]\n",
    "            \n",
    "            result = {\n",
    "                'user_id': user_id,\n",
    "                'tier': user_tier.value,\n",
    "                'allowed': True,\n",
    "                'warnings': [],\n",
    "                'quotas': {}\n",
    "            }\n",
    "            \n",
    "            # Check each quota period\n",
    "            for period in QuotaPeriod:\n",
    "                usage_record = self.get_or_create_usage_record(user_id, period)\n",
    "                quota_limit = limits[period]\n",
    "                \n",
    "                # Calculate usage percentages\n",
    "                requests_usage = (usage_record.requests_used + 1) / quota_limit.requests\n",
    "                tokens_usage = (usage_record.tokens_used + requested_tokens) / quota_limit.tokens\n",
    "                \n",
    "                quota_status = {\n",
    "                    'period': period.value,\n",
    "                    'requests': {\n",
    "                        'used': usage_record.requests_used,\n",
    "                        'limit': quota_limit.requests,\n",
    "                        'remaining': quota_limit.requests - usage_record.requests_used,\n",
    "                        'usage_percent': usage_record.requests_used / quota_limit.requests * 100\n",
    "                    },\n",
    "                    'tokens': {\n",
    "                        'used': usage_record.tokens_used,\n",
    "                        'limit': quota_limit.tokens,\n",
    "                        'remaining': quota_limit.tokens - usage_record.tokens_used,\n",
    "                        'usage_percent': usage_record.tokens_used / quota_limit.tokens * 100\n",
    "                    },\n",
    "                    'period_end': usage_record.period_end.isoformat()\n",
    "                }\\n                \n",
    "                # Check if quota would be exceeded\n",
    "                if usage_record.requests_used >= quota_limit.requests:\n",
    "                    result['allowed'] = False\n",
    "                    result['warnings'].append(f\\\"{period.value.title()} request quota exceeded\\\")\\n                    \n",
    "                elif usage_record.tokens_used + requested_tokens > quota_limit.tokens:\n",
    "                    result['allowed'] = False\n",
    "                    result['warnings'].append(f\\\"{period.value.title()} token quota would be exceeded\\\")\\n                    \n",
    "                # Soft warnings at 80% and 95%\\n                elif requests_usage >= 0.95 or tokens_usage >= 0.95:\n",
    "                    result['warnings'].append(f\\\"{period.value.title()} quota 95% used - approaching limit\\\")\\n                elif requests_usage >= 0.8 or tokens_usage >= 0.8:\n",
    "                    result['warnings'].append(f\\\"{period.value.title()} quota 80% used\\\")\\n                \n",
    "                result['quotas'][period.value] = quota_status\\n            \n",
    "            return result\\n    \n",
    "    def record_usage(self, user_id: str, tokens_used: int, success: bool = True):\\n        \\\"\\\"\\\"Record API usage for quota tracking\\\"\\\"\\\"\\n        with self.lock:\\n            for period in QuotaPeriod:\\n                usage_record = self.get_or_create_usage_record(user_id, period)\\n                usage_record.requests_used += 1\\n                if success:  # Only count tokens for successful requests\\n                    usage_record.tokens_used += tokens_used\\n                usage_record.last_updated = datetime.now()\\n            \\n            logger.info(f\\\"📊 Recorded usage for {user_id}: {tokens_used} tokens, success={success}\\\")\\n    \\n    def get_usage_summary(self, user_id: str) -> Dict[str, Any]:\\n        \\\"\\\"\\\"Get comprehensive usage summary for user\\\"\\\"\\\"\\n        with self.lock:\\n            user_tier = self.user_tiers.get(user_id, UserTier.FREE)\\n            limits = QUOTA_LIMITS[user_tier]\\n            \\n            summary = {\\n                'user_id': user_id,\\n                'tier': user_tier.value,\\n                'periods': {}\\n            }\\n            \\n            for period in QuotaPeriod:\\n                usage_record = self.get_or_create_usage_record(user_id, period)\\n                quota_limit = limits[period]\\n                \\n                summary['periods'][period.value] = {\\n                    'requests_used': usage_record.requests_used,\\n                    'requests_limit': quota_limit.requests,\\n                    'requests_remaining': max(0, quota_limit.requests - usage_record.requests_used),\\n                    'tokens_used': usage_record.tokens_used,\\n                    'tokens_limit': quota_limit.tokens,\\n                    'tokens_remaining': max(0, quota_limit.tokens - usage_record.tokens_used),\\n                    'period_start': usage_record.period_start.isoformat(),\\n                    'period_end': usage_record.period_end.isoformat(),\\n                    'days_remaining': (usage_record.period_end - datetime.now()).days\\n                }\\n            \\n            return summary\\n\\n# Initialize quota manager\\nquota_manager = UsageQuotaManager()\\n\\n# Set up test users with different tiers\\ntest_users = [\\n    ('user_free', UserTier.FREE),\\n    ('user_basic', UserTier.BASIC),\\n    ('user_premium', UserTier.PREMIUM)\\n]\\n\\nfor user_id, tier in test_users:\\n    quota_manager.set_user_tier(user_id, tier)\\n\\nprint(\\\"🎫 Testing Usage Quota System...\\\\n\\\")\\n\\n# Test quota checking for different users\\nfor user_id, tier in test_users:\\n    print(f\\\"👤 {user_id.upper()} ({tier.value.upper()}) Tier:\\\")\\n    \\n    # Check initial quota\\n    quota_check = quota_manager.check_quota(user_id, requested_tokens=5000)\\n    print(f\\\"  ✅ Allowed: {quota_check['allowed']}\\\")\\n    print(f\\\"  📊 Daily: {quota_check['quotas']['daily']['requests']['remaining']:,} requests, {quota_check['quotas']['daily']['tokens']['remaining']:,} tokens remaining\\\")\\n    \\n    if quota_check['warnings']:\\n        for warning in quota_check['warnings']:\\n            print(f\\\"  ⚠️ {warning}\\\")\\n    \\n    # Simulate some usage\\n    for i in range(3):\\n        if quota_check['allowed']:\\n            quota_manager.record_usage(user_id, tokens_used=random.randint(1000, 3000), success=True)\\n    \\n    print()\\n\\nprint(\\\"✅ Usage Quota Management system operational!\\\")\\nprint(\\\"🎯 Features: Multi-tier quotas, period tracking, soft warnings, hard limits\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6906e062",
   "metadata": {},
   "source": [
    "## 🔄 Bulk Processing with Rate Limiting\n",
    "\n",
    "Process large batches of requests efficiently while respecting API rate limits. This includes queue management, throttling, and progress tracking.\n",
    "\n",
    "### Bulk Processing Features:\n",
    "- **Queue-Based Processing**: FIFO request queuing\n",
    "- **Adaptive Throttling**: Dynamic rate adjustment\n",
    "- **Progress Tracking**: Real-time processing status\n",
    "- **Error Recovery**: Retry failed items without losing progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db3022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from asyncio import Queue, Semaphore\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Any, List\n",
    "import uuid\n",
    "\n",
    "@dataclass\n",
    "class BulkRequest:\n",
    "    \"\"\"Individual request in bulk processing queue\"\"\"\n",
    "    id: str\n",
    "    data: Any\n",
    "    priority: int = 0\n",
    "    retries: int = 0\n",
    "    max_retries: int = 3\n",
    "    estimated_tokens: int = 1000\n",
    "\n",
    "@dataclass\n",
    "class ProcessingResult:\n",
    "    \"\"\"Result of processing a bulk request\"\"\"\n",
    "    request_id: str\n",
    "    success: bool\n",
    "    result: Any = None\n",
    "    error: Exception = None\n",
    "    tokens_used: int = 0\n",
    "    processing_time: float = 0\n",
    "\n",
    "class BulkProcessor:\n",
    "    \"\"\"\n",
    "    Rate-limited bulk processing system with queue management\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 rate_monitor: RateLimitMonitor,\n",
    "                 max_concurrent: int = 5,\n",
    "                 batch_size: int = 10,\n",
    "                 progress_callback: Callable = None):\n",
    "        \n",
    "        self.rate_monitor = rate_monitor\n",
    "        self.max_concurrent = max_concurrent\n",
    "        self.batch_size = batch_size\n",
    "        self.progress_callback = progress_callback\n",
    "        \n",
    "        # Processing queue and semaphore\n",
    "        self.request_queue: Queue = Queue()\n",
    "        self.result_queue: Queue = Queue()\n",
    "        self.semaphore = Semaphore(max_concurrent)\n",
    "        \n",
    "        # Processing statistics\n",
    "        self.total_items = 0\n",
    "        self.completed_items = 0\n",
    "        self.failed_items = 0\n",
    "        self.processing_start_time = None\n",
    "        \n",
    "        # Queue for failed items (for retry)\n",
    "        self.retry_queue: Queue = Queue()\n",
    "        \n",
    "    async def add_requests(self, requests: List[BulkRequest]):\n",
    "        \"\"\"Add requests to processing queue\"\"\"\n",
    "        for request in requests:\n",
    "            await self.request_queue.put(request)\n",
    "            self.total_items += 1\n",
    "        \n",
    "        logger.info(f\\\"📥 Added {len(requests)} requests to bulk processing queue\\\")\\n    \\n    async def process_single_request(self, request: BulkRequest, processor_func: Callable) -> ProcessingResult:\\n        \\\"\\\"\\\"Process a single request with rate limiting and error handling\\\"\\\"\\\"\\n        async with self.semaphore:\\n            start_time = time.time()\\n            \\n            try:\\n                # Check rate limits before processing\\n                while True:\\n                    can_request = self.rate_monitor.can_make_request(\\n                        estimated_tokens=request.estimated_tokens\\n                    )\\n                    \\n                    if can_request['allowed']:\\n                        break\\n                    else:\\n                        wait_time = can_request.get('wait_time', 1)\\n                        logger.info(f\\\"⏱️ Rate limit reached, waiting {wait_time:.1f}s...\\\")\\n                        await asyncio.sleep(wait_time)\\n                \\n                # Process the request\\n                logger.debug(f\\\"🔄 Processing request {request.id}...\\\")\\n                result = await processor_func(request.data)\\n                \\n                # Record successful processing\\n                processing_time = time.time() - start_time\\n                tokens_used = getattr(result, 'tokens_used', request.estimated_tokens)\\n                \\n                self.rate_monitor.record_request(\\n                    tokens_used=tokens_used,\\n                    success=True\\n                )\\n                \\n                self.completed_items += 1\\n                \\n                return ProcessingResult(\\n                    request_id=request.id,\\n                    success=True,\\n                    result=result,\\n                    tokens_used=tokens_used,\\n                    processing_time=processing_time\\n                )\\n                \\n            except Exception as error:\\n                processing_time = time.time() - start_time\\n                \\n                # Record failed processing\\n                self.rate_monitor.record_request(\\n                    tokens_used=0,\\n                    success=False\\n                )\\n                \\n                # Determine if we should retry\\n                if request.retries < request.max_retries:\\n                    smart_handler = SmartRetryHandler()\\n                    if smart_handler.backoff.should_retry(error, request.retries):\\n                        request.retries += 1\\n                        await self.retry_queue.put(request)\\n                        logger.warning(f\\\"🔄 Queuing request {request.id} for retry ({request.retries}/{request.max_retries})\\\")\\n                        return None  # Will be retried\\n                \\n                # Mark as permanently failed\\n                self.failed_items += 1\\n                logger.error(f\\\"❌ Request {request.id} failed permanently: {error}\\\")\\n                \\n                return ProcessingResult(\\n                    request_id=request.id,\\n                    success=False,\\n                    error=error,\\n                    processing_time=processing_time\\n                )\\n    \\n    async def process_batch(self, processor_func: Callable) -> List[ProcessingResult]:\\n        \\\"\\\"\\\"Process all requests in the queue\\\"\\\"\\\"\\n        self.processing_start_time = time.time()\\n        results = []\\n        \\n        logger.info(f\\\"🚀 Starting bulk processing of {self.total_items} items...\\\")\\n        \\n        # Create worker tasks\\n        workers = []\\n        for i in range(self.max_concurrent):\\n            worker = asyncio.create_task(self._worker(processor_func, results))\\n            workers.append(worker)\\n        \\n        # Wait for all items to be processed\\n        await self.request_queue.join()\\n        \\n        # Process retry queue\\n        await self._process_retries(processor_func, results)\\n        \\n        # Cancel workers\\n        for worker in workers:\\n            worker.cancel()\\n        \\n        # Calculate final statistics\\n        total_time = time.time() - self.processing_start_time\\n        success_rate = (self.completed_items / self.total_items) * 100 if self.total_items > 0 else 0\\n        \\n        logger.info(f\\\"✅ Bulk processing completed:\\\")\\n        logger.info(f\\\"   📊 Total items: {self.total_items}\\\")\\n        logger.info(f\\\"   ✅ Successful: {self.completed_items}\\\")\\n        logger.info(f\\\"   ❌ Failed: {self.failed_items}\\\")\\n        logger.info(f\\\"   📈 Success rate: {success_rate:.1f}%\\\")\\n        logger.info(f\\\"   ⏱️ Total time: {total_time:.2f}s\\\")\\n        logger.info(f\\\"   🚀 Throughput: {self.total_items/total_time:.1f} items/s\\\")\\n        \\n        return results\\n    \\n    async def _worker(self, processor_func: Callable, results: List[ProcessingResult]):\\n        \\\"\\\"\\\"Worker coroutine to process requests from queue\\\"\\\"\\\"\\n        while True:\\n            try:\\n                request = await self.request_queue.get()\\n                result = await self.process_single_request(request, processor_func)\\n                \\n                if result:  # Only add non-None results (retries return None)\\n                    results.append(result)\\n                \\n                # Update progress\\n                if self.progress_callback:\\n                    progress = (self.completed_items + self.failed_items) / self.total_items\\n                    await self.progress_callback(progress, self.completed_items, self.failed_items)\\n                \\n                self.request_queue.task_done()\\n                \\n            except asyncio.CancelledError:\\n                break\\n            except Exception as error:\\n                logger.error(f\\\"❌ Worker error: {error}\\\")\\n                self.request_queue.task_done()\\n    \\n    async def _process_retries(self, processor_func: Callable, results: List[ProcessingResult]):\\n        \\\"\\\"\\\"Process items in retry queue\\\"\\\"\\\"\\n        retry_count = self.retry_queue.qsize()\\n        if retry_count > 0:\\n            logger.info(f\\\"🔄 Processing {retry_count} retry requests...\\\")\\n            \\n            while not self.retry_queue.empty():\\n                request = await self.retry_queue.get()\\n                # Add exponential backoff delay for retries\\n                backoff_delay = min(2.0 ** request.retries, 10.0)  # Max 10s delay\\n                await asyncio.sleep(backoff_delay)\\n                \\n                result = await self.process_single_request(request, processor_func)\\n                if result:\\n                    results.append(result)\\n\\n# Example usage with mock API processor\\nasync def mock_api_processor(data):\\n    \\\"\\\"\\\"Mock API processor function\\\"\\\"\\\"\\n    # Simulate API processing time\\n    await asyncio.sleep(random.uniform(0.1, 0.5))\\n    \\n    # Simulate occasional failures (10% failure rate)\\n    if random.random() < 0.1:\\n        raise Exception(\\\"Random API error\\\")\\n    \\n    # Return mock result with token usage\\n    class MockResult:\\n        def __init__(self):\\n            self.tokens_used = random.randint(500, 1500)\\n            self.data = f\\\"Processed: {data}\\\"\\n    \\n    return MockResult()\\n\\nasync def progress_tracker(progress: float, completed: int, failed: int):\\n    \\\"\\\"\\\"Progress tracking callback\\\"\\\"\\\"\\n    if completed % 5 == 0 or progress >= 1.0:  # Update every 5 items or at completion\\n        print(f\\\"📈 Progress: {progress:.1%} ({completed} completed, {failed} failed)\\\")\\n\\n# Test bulk processing system\\nasync def test_bulk_processing():\\n    # Create bulk processor with rate limiting\\n    config = RateLimitConfig(requests_per_minute=30, tokens_per_minute=50000)\\n    rate_monitor = RateLimitMonitor(config)\\n    \\n    processor = BulkProcessor(\\n        rate_monitor=rate_monitor,\\n        max_concurrent=3,\\n        batch_size=5,\\n        progress_callback=progress_tracker\\n    )\\n    \\n    # Create test requests\\n    test_requests = [\\n        BulkRequest(\\n            id=f\\\"req_{i:03d}\\\",\\n            data=f\\\"Test data item {i}\\\",\\n            estimated_tokens=random.randint(800, 1200),\\n            priority=random.randint(1, 3)\\n        )\\n        for i in range(20)\\n    ]\\n    \\n    # Add requests to processor\\n    await processor.add_requests(test_requests)\\n    \\n    # Process all requests\\n    results = await processor.process_batch(mock_api_processor)\\n    \\n    # Analyze results\\n    successful_results = [r for r in results if r.success]\\n    failed_results = [r for r in results if not r.success]\\n    \\n    print(f\\\"\\\\n📊 Final Results Summary:\\\")\\n    print(f\\\"   ✅ Successful requests: {len(successful_results)}\\\")\\n    print(f\\\"   ❌ Failed requests: {len(failed_results)}\\\")\\n    \\n    if successful_results:\\n        avg_processing_time = sum(r.processing_time for r in successful_results) / len(successful_results)\\n        total_tokens = sum(r.tokens_used for r in successful_results)\\n        print(f\\\"   ⏱️ Average processing time: {avg_processing_time:.2f}s\\\")\\n        print(f\\\"   🪙 Total tokens used: {total_tokens:,}\\\")\\n    \\n    # Show rate limit status\\n    status = rate_monitor.get_status()\\n    print(f\\\"\\\\n📈 Final Rate Limit Status:\\\")\\n    print(f\\\"   Requests used: {status['global']['requests_per_minute']}/{config.requests_per_minute}\\\")\\n    print(f\\\"   Tokens used: {status['global']['tokens_per_minute']:,}/{config.tokens_per_minute:,}\\\")\\n    \\n    return results\\n\\nprint(\\\"🔄 Bulk Processing System initialized!\\\")\\nprint(\\\"🧪 Run 'await test_bulk_processing()' to test the system\\\")\\nprint(\\\"✨ Features: Queue management, rate limiting, retry logic, progress tracking\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5961e1",
   "metadata": {},
   "source": [
    "## 🛡️ Error Handling Best Practices\n",
    "\n",
    "Implement comprehensive error handling patterns including logging, graceful degradation, and recovery strategies for production-ready systems.\n",
    "\n",
    "### Best Practices:\n",
    "- **Structured Logging**: Detailed error context and metrics\n",
    "- **Circuit Breaker Pattern**: Prevent cascading failures\n",
    "- **Graceful Degradation**: Fallback to reduced functionality\n",
    "- **Health Monitoring**: Proactive system health checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d0150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import json\n",
    "from contextlib import contextmanager\n",
    "from functools import wraps\n",
    "\n",
    "class CircuitState(Enum):\n",
    "    CLOSED = \"closed\"      # Normal operation\n",
    "    OPEN = \"open\"          # Failing, rejecting requests\n",
    "    HALF_OPEN = \"half_open\" # Testing if service recovered\n",
    "\n",
    "class HealthStatus(Enum):\n",
    "    HEALTHY = \"healthy\"\n",
    "    DEGRADED = \"degraded\"\n",
    "    UNHEALTHY = \"unhealthy\"\n",
    "\n",
    "class CircuitBreaker:\n",
    "    \"\"\"\n",
    "    Circuit breaker pattern implementation for API resilience\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 failure_threshold: int = 5,\n",
    "                 recovery_timeout: int = 60,\n",
    "                 expected_exception: type = Exception):\n",
    "        \n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.recovery_timeout = recovery_timeout\n",
    "        self.expected_exception = expected_exception\n",
    "        \n",
    "        # Circuit state\n",
    "        self.state = CircuitState.CLOSED\n",
    "        self.failure_count = 0\n",
    "        self.last_failure_time = None\n",
    "        self.success_count = 0\n",
    "        \n",
    "    def can_execute(self) -> bool:\n",
    "        \\\"\\\"\\\"Check if request can be executed based on circuit state\\\"\\\"\\\"\\n        if self.state == CircuitState.CLOSED:\\n            return True\\n        \\n        if self.state == CircuitState.OPEN:\\n            # Check if recovery timeout has elapsed\\n            if time.time() - self.last_failure_time >= self.recovery_timeout:\\n                self.state = CircuitState.HALF_OPEN\\n                self.success_count = 0\\n                logger.info(\\\"🔄 Circuit breaker entering HALF_OPEN state\\\")\\n                return True\\n            return False\\n        \\n        # HALF_OPEN state - allow limited requests to test recovery\\n        return True\\n    \\n    def record_success(self):\\n        \\\"\\\"\\\"Record successful operation\\\"\\\"\\\"\\n        self.failure_count = 0\\n        \\n        if self.state == CircuitState.HALF_OPEN:\\n            self.success_count += 1\\n            # After 3 successful requests, close the circuit\\n            if self.success_count >= 3:\\n                self.state = CircuitState.CLOSED\\n                logger.info(\\\"✅ Circuit breaker returning to CLOSED state\\\")\\n    \\n    def record_failure(self):\\n        \\\"\\\"\\\"Record failed operation\\\"\\\"\\\"\\n        self.failure_count += 1\\n        self.last_failure_time = time.time()\\n        \\n        if self.failure_count >= self.failure_threshold:\\n            self.state = CircuitState.OPEN\\n            logger.warning(f\\\"⚠️ Circuit breaker OPENED after {self.failure_count} failures\\\")\\n    \\n    def get_state(self) -> dict:\\n        \\\"\\\"\\\"Get current circuit breaker state\\\"\\\"\\\"\\n        return {\\n            'state': self.state.value,\\n            'failure_count': self.failure_count,\\n            'success_count': self.success_count,\\n            'failure_threshold': self.failure_threshold,\\n            'last_failure_time': self.last_failure_time,\\n            'can_execute': self.can_execute()\\n        }\\n\\nclass StructuredLogger:\\n    \\\"\\\"\\\"Enhanced logging with structured data and context\\\"\\\"\\\"\\n    \\n    def __init__(self, name: str):\\n        self.logger = logging.getLogger(name)\\n        self.context = {}\\n    \\n    def set_context(self, **kwargs):\\n        \\\"\\\"\\\"Set logging context\\\"\\\"\\\"\\n        self.context.update(kwargs)\\n    \\n    def clear_context(self):\\n        \\\"\\\"\\\"Clear logging context\\\"\\\"\\\"\\n        self.context.clear()\\n    \\n    def _log_with_context(self, level, message, **kwargs):\\n        \\\"\\\"\\\"Log message with context\\\"\\\"\\\"\\n        log_data = {\\n            'message': message,\\n            'timestamp': datetime.now().isoformat(),\\n            'context': self.context.copy(),\\n            **kwargs\\n        }\\n        \\n        formatted_message = f\\\"{message} | Context: {json.dumps(log_data, indent=None)}\\\"\\n        getattr(self.logger, level)(formatted_message)\\n    \\n    def info(self, message, **kwargs):\\n        self._log_with_context('info', message, **kwargs)\\n    \\n    def warning(self, message, **kwargs):\\n        self._log_with_context('warning', message, **kwargs)\\n    \\n    def error(self, message, **kwargs):\\n        self._log_with_context('error', message, **kwargs)\\n    \\n    def debug(self, message, **kwargs):\\n        self._log_with_context('debug', message, **kwargs)\\n\\n@contextmanager\\ndef error_context(structured_logger: StructuredLogger, operation: str, **context):\\n    \\\"\\\"\\\"Context manager for error handling with structured logging\\\"\\\"\\\"\\n    structured_logger.set_context(operation=operation, **context)\\n    start_time = time.time()\\n    \\n    try:\\n        structured_logger.info(f\\\"Starting {operation}\\\")\\n        yield structured_logger\\n        \\n        duration = time.time() - start_time\\n        structured_logger.info(f\\\"Completed {operation}\\\", duration=duration, status=\\\"success\\\")\\n        \\n    except Exception as error:\\n        duration = time.time() - start_time\\n        structured_logger.error(\\n            f\\\"Failed {operation}: {str(error)}\\\",\\n            duration=duration,\\n            status=\\\"error\\\",\\n            error_type=type(error).__name__,\\n            error_message=str(error)\\n        )\\n        raise\\n    \\n    finally:\\n        structured_logger.clear_context()\\n\\ndef with_circuit_breaker(circuit_breaker: CircuitBreaker, fallback_func: Callable = None):\\n    \\\"\\\"\\\"Decorator to apply circuit breaker pattern\\\"\\\"\\\"\\n    def decorator(func):\\n        @wraps(func)\\n        async def wrapper(*args, **kwargs):\\n            # Check if circuit allows execution\\n            if not circuit_breaker.can_execute():\\n                if fallback_func:\\n                    logger.info(\\\"🔄 Circuit breaker OPEN, using fallback\\\")\\n                    return await fallback_func(*args, **kwargs) if asyncio.iscoroutinefunction(fallback_func) else fallback_func(*args, **kwargs)\\n                else:\\n                    raise Exception(\\\"Circuit breaker OPEN - service temporarily unavailable\\\")\\n            \\n            try:\\n                result = await func(*args, **kwargs) if asyncio.iscoroutinefunction(func) else func(*args, **kwargs)\\n                circuit_breaker.record_success()\\n                return result\\n            \\n            except circuit_breaker.expected_exception as error:\\n                circuit_breaker.record_failure()\\n                raise\\n        \\n        return wrapper\\n    return decorator\\n\\nclass HealthMonitor:\\n    \\\"\\\"\\\"System health monitoring and alerting\\\"\\\"\\\"\\n    \\n    def __init__(self):\\n        self.health_checks = {}\\n        self.overall_status = HealthStatus.HEALTHY\\n        self.last_check_time = None\\n    \\n    def register_check(self, name: str, check_func: Callable, threshold: float = 0.8):\\n        \\\"\\\"\\\"Register a health check function\\\"\\\"\\\"\\n        self.health_checks[name] = {\\n            'function': check_func,\\n            'threshold': threshold,\\n            'last_result': None,\\n            'last_check': None\\n        }\\n    \\n    async def run_health_checks(self) -> dict:\\n        \\\"\\\"\\\"Run all registered health checks\\\"\\\"\\\"\\n        results = {}\\n        healthy_checks = 0\\n        total_checks = len(self.health_checks)\\n        \\n        for name, check_config in self.health_checks.items():\\n            try:\\n                start_time = time.time()\\n                \\n                if asyncio.iscoroutinefunction(check_config['function']):\\n                    result = await check_config['function']()\\n                else:\\n                    result = check_config['function']()\\n                \\n                check_duration = time.time() - start_time\\n                \\n                # Determine if check passed based on threshold\\n                if isinstance(result, (int, float)):\\n                    passed = result >= check_config['threshold']\\n                    score = result\\n                else:\\n                    passed = bool(result)\\n                    score = 1.0 if passed else 0.0\\n                \\n                if passed:\\n                    healthy_checks += 1\\n                \\n                results[name] = {\\n                    'passed': passed,\\n                    'score': score,\\n                    'threshold': check_config['threshold'],\\n                    'duration': check_duration,\\n                    'timestamp': datetime.now().isoformat()\\n                }\\n                \\n                check_config['last_result'] = results[name]\\n                check_config['last_check'] = time.time()\\n                \\n            except Exception as error:\\n                results[name] = {\\n                    'passed': False,\\n                    'error': str(error),\\n                    'timestamp': datetime.now().isoformat()\\n                }\\n        \\n        # Determine overall health status\\n        health_ratio = healthy_checks / total_checks if total_checks > 0 else 0\\n        \\n        if health_ratio >= 0.9:\\n            self.overall_status = HealthStatus.HEALTHY\\n        elif health_ratio >= 0.7:\\n            self.overall_status = HealthStatus.DEGRADED\\n        else:\\n            self.overall_status = HealthStatus.UNHEALTHY\\n        \\n        self.last_check_time = time.time()\\n        \\n        return {\\n            'overall_status': self.overall_status.value,\\n            'health_ratio': health_ratio,\\n            'total_checks': total_checks,\\n            'healthy_checks': healthy_checks,\\n            'checks': results,\\n            'timestamp': datetime.now().isoformat()\\n        }\\n    \\n    def get_status(self) -> dict:\\n        \\\"\\\"\\\"Get current health status\\\"\\\"\\\"\\n        return {\\n            'status': self.overall_status.value,\\n            'last_check': self.last_check_time,\\n            'registered_checks': list(self.health_checks.keys())\\n        }\\n\\n# Example usage and testing\\n\\n# Initialize components\\ncircuit_breaker = CircuitBreaker(failure_threshold=3, recovery_timeout=10)\\nstructured_logger = StructuredLogger('error_mitigation')\\nhealth_monitor = HealthMonitor()\\n\\n# Mock health check functions\\ndef check_openai_api():\\n    \\\"\\\"\\\"Mock OpenAI API health check\\\"\\\"\\\"\\n    # Simulate API health based on recent success rate\\n    return random.uniform(0.7, 1.0)  # 70-100% success rate\\n\\ndef check_database_connection():\\n    \\\"\\\"\\\"Mock database health check\\\"\\\"\\\"\\n    return random.choice([True, True, True, False])  # 75% success rate\\n\\nasync def check_rate_limits():\\n    \\\"\\\"\\\"Mock rate limit health check\\\"\\\"\\\"\\n    # Simulate checking rate limit utilization\\n    await asyncio.sleep(0.1)  # Simulate async operation\\n    return random.uniform(0.5, 0.9)  # 50-90% utilization\\n\\n# Register health checks\\nhealth_monitor.register_check('openai_api', check_openai_api, threshold=0.8)\\nhealth_monitor.register_check('database', check_database_connection, threshold=0.9)\\nhealth_monitor.register_check('rate_limits', check_rate_limits, threshold=0.95)\\n\\n# Example API function with circuit breaker\\n@with_circuit_breaker(circuit_breaker)\\nasync def example_api_call(data):\\n    \\\"\\\"\\\"Example API call with circuit breaker protection\\\"\\\"\\\"\\n    # Simulate occasional failures\\n    if random.random() < 0.3:  # 30% failure rate for testing\\n        raise Exception(\\\"API temporarily unavailable\\\")\\n    \\n    return f\\\"Processed: {data}\\\"\\n\\n# Test the error handling system\\nasync def test_error_handling():\\n    print(\\\"🧪 Testing Error Handling Best Practices...\\\\n\\\")\\n    \\n    # Test structured logging with context\\n    with error_context(structured_logger, \\\"test_operation\\\", user_id=\\\"test_user\\\", batch_id=\\\"batch_123\\\"):\\n        structured_logger.info(\\\"Processing batch operation\\\")\\n        await asyncio.sleep(0.1)\\n        # Operation completes successfully\\n    \\n    # Test circuit breaker\\n    print(\\\"🔧 Testing Circuit Breaker Pattern:\\\")\\n    for i in range(8):\\n        try:\\n            result = await example_api_call(f\\\"test_data_{i}\\\")\\n            print(f\\\"  Request {i+1}: ✅ {result}\\\")\\n        except Exception as error:\\n            print(f\\\"  Request {i+1}: ❌ {error}\\\")\\n        \\n        # Show circuit state\\n        state = circuit_breaker.get_state()\\n        if state['state'] != 'closed':\\n            print(f\\\"    Circuit State: {state['state'].upper()} (failures: {state['failure_count']})\\\")\\n    \\n    # Test health monitoring\\n    print(\\\"\\\\n🏥 Running Health Checks:\\\")\\n    health_results = await health_monitor.run_health_checks()\\n    \\n    print(f\\\"Overall Status: {health_results['overall_status'].upper()}\\\")\\n    print(f\\\"Health Ratio: {health_results['health_ratio']:.1%}\\\")\\n    \\n    for check_name, result in health_results['checks'].items():\\n        status = \\\"✅\\\" if result['passed'] else \\\"❌\\\"\\n        if 'score' in result:\\n            print(f\\\"  {status} {check_name}: {result['score']:.2f} (threshold: {result['threshold']})\\\")\\n        else:\\n            print(f\\\"  {status} {check_name}: {result.get('error', 'Unknown error')}\\\")\\n    \\n    return health_results\\n\\nprint(\\\"🛡️ Error Handling Best Practices System initialized!\\\")\\nprint(\\\"🧪 Run 'await test_error_handling()' to test the system\\\")\\nprint(\\\"\\\\n✨ Implemented Features:\\\")\\nprint(\\\"  🔄 Circuit Breaker Pattern\\\")\\nprint(\\\"  📊 Structured Logging with Context\\\")\\nprint(\\\"  🏥 Health Monitoring System\\\")\\nprint(\\\"  ⚠️ Graceful Error Handling\\\")\\nprint(\\\"  📈 Comprehensive Error Analytics\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056438e0",
   "metadata": {},
   "source": [
    "## 📋 Summary & Implementation Checklist\n",
    "\n",
    "This notebook has demonstrated comprehensive error mitigation strategies for production OpenAI API integration. Here's your implementation roadmap:\n",
    "\n",
    "### ✅ Immediate Actions (High Impact, Low Effort)\n",
    "\n",
    "1. **Implement Exponential Backoff**\n",
    "   - Add retry logic with exponential delays\n",
    "   - Include random jitter to prevent thundering herd\n",
    "   - Set maximum retry limits and timeouts\n",
    "\n",
    "2. **Add Rate Limit Monitoring** \n",
    "   - Track requests per minute and tokens per minute\n",
    "   - Implement proactive rate limiting before hitting API limits\n",
    "   - Add usage dashboards and alerts\n",
    "\n",
    "3. **Set Up Usage Quotas**\n",
    "   - Define user tiers with different limits\n",
    "   - Implement daily/weekly/monthly quotas\n",
    "   - Add soft warnings at 80% and hard stops at 100%\n",
    "\n",
    "### 🔧 Medium-Term Improvements\n",
    "\n",
    "4. **Deploy Circuit Breaker Pattern**\n",
    "   - Prevent cascading failures during outages  \n",
    "   - Implement automatic recovery testing\n",
    "   - Add fallback mechanisms for degraded service\n",
    "\n",
    "5. **Enhance Error Logging**\n",
    "   - Structured logging with context\n",
    "   - Error classification and analytics\n",
    "   - Performance monitoring and alerts\n",
    "\n",
    "6. **Bulk Processing Optimization**\n",
    "   - Queue-based request processing\n",
    "   - Adaptive throttling based on API response\n",
    "   - Progress tracking and retry mechanisms\n",
    "\n",
    "### 🚀 Advanced Features\n",
    "\n",
    "7. **Health Monitoring System**\n",
    "   - Automated health checks for all dependencies\n",
    "   - Real-time status dashboard\n",
    "   - Predictive failure detection\n",
    "\n",
    "8. **Cost Optimization**\n",
    "   - Model selection based on task complexity\n",
    "   - Token usage optimization\n",
    "   - Batch processing for efficiency\n",
    "\n",
    "### 💡 Key Takeaways\n",
    "\n",
    "- **Proactive > Reactive**: Prevent errors rather than just handling them\n",
    "- **Monitor Everything**: Track usage, performance, and costs continuously  \n",
    "- **Graceful Degradation**: Maintain partial functionality during failures\n",
    "- **User Experience**: Transparent communication about limits and issues\n",
    "- **Cost Control**: Balance functionality with API usage costs\n",
    "\n",
    "### 🔗 Integration Steps\n",
    "\n",
    "To integrate these patterns into your existing system:\n",
    "\n",
    "1. **Start with basic exponential backoff** in your current API calls\n",
    "2. **Add rate limiting** to prevent quota exceeded errors  \n",
    "3. **Implement usage tracking** for different user tiers\n",
    "4. **Deploy health checks** for proactive monitoring\n",
    "5. **Scale up** with bulk processing and circuit breakers\n",
    "\n",
    "Remember: **Start simple, iterate quickly, monitor continuously!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
