/**
 * Workflow Orchestration Service
 * 
 * Manages multi-stage background job processing with Redis:
 * 1. File Upload → AI Parse
 * 2. AI Parse → Database Save  
 * 3. Database Save → Shopify Sync (if approved)
 * 4. Shopify Sync → Update PO Status → Complete
 * 
 * Each stage is tracked with comprehensive metadata for monitoring an        cons                //        if        if (        // Convert buffer to string
        contentForProcessing = fileResult.buffer.toString('utf-8');
        console.log('📄 Downloaded file content, length:', contentForProcessing.length);ileResult.success) {
          throw new Error(`Failed to download file: ${fileResult.error}`);
        }
        
        // Convert buffer to string
        contentForProcessing = fileResult.buffer.toString('utf-8');
        console.log('📄 Downloaded file content, length:', contentForProcessing.length);
        
      } catch (error) {  // Convert buffer to string  
        contentForProcessing = fileResult.buffer.toString('utf-8');
        console.log('📄 Downloaded file content, length:', contentForProcessing.length);ileResult.success) {
          throw new Error(`Failed to download file: ${fileResult.error}`);
        }
        
        // Convert buffer to string
        contentForProcessing = fileResult.buffer.toString('utf-8');
        console.log('📄 Downloaded file content, length:', contentForProcessing.length);
        
      } catch (error) {d file content
        const fileResult = await storageService.downloadFile(upload.fileUrl);
        if (!fileResult.success) {
          throw new Error(`Failed to download file: ${fileResult.error}`);
        }
        
        // Convert buffer to string
        contentForProcessing = fileResult.buffer.toString('utf-8');
        console.log('📄 Downloaded file content, length:', contentForProcessing.length);oad file content
        const fileResult = await storageService.downloadFile(upload.fileUrl);
        if (!fileResult.success) {
          throw new Error(`Failed to download file: ${fileResult.error}`);
        }
        
        // Convert buffer to string
        contentForProcessing = fileResult.buffer.toString('utf-8');
        console.log('📄 Downloaded file content, length:', contentForProcessing.length);'📁 File URL found:', upload.fileUrl);
        
        // Download file content
        const fileResult = await storageService.downloadFile(upload.fileUrl);
        if (!fileResult.success) {
          throw new Error(`Failed to download file: ${fileResult.error}`);
        }
        
        // Convert buffer to string
        contentForProcessing = fileResult.buffer.toString('utf-8');
        console.log('📄 Downloaded file content, length:', contentForProcessing.length);g.
 */

import Bull from 'bull'
import redisManager from './redisManager.js'
import { getRedisConfig } from '../config/redis.production.js'
import { enhancedAIService } from './enhancedAIService.js'
import { enhancedShopifyService } from './enhancedShopifyService.js'
import { errorHandlingService, MERCHANT_MESSAGES } from './errorHandlingService.js'
import { DatabasePersistenceService } from './databasePersistenceService.js'
import { db } from './db.js'
import { processorRegistrationService } from './processorRegistrationService.js'

// Workflow Stage Definitions
export const WORKFLOW_STAGES = {
  FILE_UPLOAD: 'file_upload',
  AI_PARSING: 'ai_parsing', 
  DATABASE_SAVE: 'database_save',
  SHOPIFY_SYNC: 'shopify_sync',
  STATUS_UPDATE: 'status_update',
  COMPLETED: 'completed',
  FAILED: 'failed'
}

// Job Types for different queues
export const JOB_TYPES = {
  AI_PARSE: 'ai_parse',
  DATABASE_SAVE: 'database_save',
  SHOPIFY_SYNC: 'shopify_sync',
  STATUS_UPDATE: 'status_update'
}

// Job Status Tracking
export const JOB_STATUS = {
  PENDING: 'pending',
  PROCESSING: 'processing',
  COMPLETED: 'completed',
  FAILED: 'failed',
  RETRYING: 'retrying',
  CANCELLED: 'cancelled'
}

export class WorkflowOrchestrator {
  constructor() {
    this.queues = new Map()
    this.redisManager = redisManager // Use singleton instance
    this.isInitialized = false
    
    // Workflow configuration
    this.config = {
      retryAttempts: 3,
      retryDelay: 5000, // 5 seconds
      jobTimeout: 300000, // 5 minutes default
      concurrency: {
        ai_parse: 2,
        database_save: 5,
        shopify_sync: 3,
        status_update: 10
      }
    }
    
    // Statistics tracking
    this.stats = {
      workflows: {
        started: 0,
        completed: 0,
        failed: 0
      },
      stages: {
        [WORKFLOW_STAGES.AI_PARSING]: { processed: 0, failed: 0 },
        [WORKFLOW_STAGES.DATABASE_SAVE]: { processed: 0, failed: 0 },
        [WORKFLOW_STAGES.SHOPIFY_SYNC]: { processed: 0, failed: 0 },
        [WORKFLOW_STAGES.STATUS_UPDATE]: { processed: 0, failed: 0 }
      }
    }
  }

  /**
   * Initialize all workflow queues
   */
  async initialize() {
    if (this.isInitialized) return

    try {
      console.log('🚀 Initializing Workflow Orchestrator...')
      
      await this.redisManager.waitForConnection(15000)
      
      const environment = process.env.NODE_ENV || 'development'
      const redisConfig = getRedisConfig(environment)
      const connectionOptions = redisConfig.connection

      // Create specialized queues for each workflow stage
      const queueConfigs = [
        {
          name: 'ai-parsing',
          type: JOB_TYPES.AI_PARSE,
          concurrency: this.config.concurrency.ai_parse,
          timeout: 600000 // 10 minutes for AI processing
        },
        {
          name: 'database-save',
          type: JOB_TYPES.DATABASE_SAVE,
          concurrency: this.config.concurrency.database_save,
          timeout: 60000 // 1 minute for DB operations
        },
        {
          name: 'shopify-sync',
          type: JOB_TYPES.SHOPIFY_SYNC,
          concurrency: this.config.concurrency.shopify_sync,
          timeout: 300000 // 5 minutes for Shopify operations
        },
        {
          name: 'status-update',
          type: JOB_TYPES.STATUS_UPDATE,
          concurrency: this.config.concurrency.status_update,
          timeout: 30000 // 30 seconds for status updates
        }
      ]

      for (const queueConfig of queueConfigs) {
        const queue = new Bull(queueConfig.name, {
          redis: connectionOptions,
          defaultJobOptions: {
            attempts: this.config.retryAttempts,
            backoff: {
              type: 'exponential',
              delay: this.config.retryDelay
            },
            removeOnComplete: 100,
            removeOnFail: 50,
            timeout: queueConfig.timeout
          },
          settings: {
            stalledInterval: 30000,
            maxStalledCount: 1
          }
        })

        console.log(`🔧 Created Bull queue: ${queueConfig.name}`)

        // Setup event handlers
        this.setupQueueEvents(queue, queueConfig.type)
        
        this.queues.set(queueConfig.type, queue)
        
        // Log queue state after setup
        const waiting = await queue.getWaiting()
        const active = await queue.getActive()
        const completed = await queue.getCompleted()
        const failed = await queue.getFailed()
        
        console.log(`📊 Queue ${queueConfig.name} state: waiting=${waiting.length}, active=${active.length}, completed=${completed.length}, failed=${failed.length}`)
        
        console.log(`✅ Initialized ${queueConfig.name} queue (concurrency: ${queueConfig.concurrency})`)
      }

      // Initialize all processors using the permanent fix
      console.log('🚀 Initializing processors with permanent fix...')
      await processorRegistrationService.initializeAllProcessors(this)

      this.isInitialized = true
      console.log('🎉 Workflow Orchestrator initialized successfully')
      
    } catch (error) {
      console.error('❌ Failed to initialize Workflow Orchestrator:', error)
      throw error
    }
  }

  /**
   * Setup queue processor for specific job type (DISABLED - using ProcessorRegistrationService)
   */
  async setupQueueProcessor(queue, jobType, concurrency) {
    console.log(`🚫 setupQueueProcessor is disabled - using ProcessorRegistrationService instead`);
    
    // This function is intentionally disabled due to previous corruption
    // All processor registration is now handled by processorRegistrationService.js
    return;
  }

  /**
   * Setup event handlers for queue monitoring
   */
  setupQueueEvents(queue, jobType) {
    queue.on('completed', (job, result) => {
      console.log(`✅ ${jobType} job ${job.id} completed`)
      if (job.data?.workflowId && job.data?.stage) {
        this.updateJobMetadata(job.data.workflowId, job.data.stage, JOB_STATUS.COMPLETED, { result })
        this.stats.stages[job.data.stage].processed++
      }
    })

    queue.on('failed', (job, err) => {
      console.error(`❌ ${jobType} job ${job.id} failed:`, err.message)
      if (job.data?.workflowId && job.data?.stage) {
        this.updateJobMetadata(job.data.workflowId, job.data.stage, JOB_STATUS.FAILED, { error: err.message })
        this.stats.stages[job.data.stage].failed++
      }
    })

    queue.on('stalled', (job) => {
      console.warn(`⚠️ ${jobType} job ${job.id} stalled`)
      if (job.data?.workflowId && job.data?.stage) {
        this.updateJobMetadata(job.data.workflowId, job.data.stage, JOB_STATUS.RETRYING)
      }
    })

    queue.on('progress', (job, progress) => {
      console.log(`📊 ${jobType} job ${job.id} progress: ${progress}%`)
      if (job.data?.workflowId && job.data?.stage) {
        this.updateJobMetadata(job.data.workflowId, job.data.stage, JOB_STATUS.PROCESSING, { progress })
      }
    })
  }

  /**
   * Process individual job based on type
   */
  async processJob(job, jobType) {
    console.log(`🔬 processJob called: jobType=${jobType}, jobId=${job.id}`);
    
    const { workflowId, stage, data } = job.data
    
    console.log(`🔄 Processing ${jobType} job for workflow ${workflowId}`)
    
    try {
      // Update job status to processing
      if (workflowId && stage) {
        console.log(`📝 Updating job metadata for workflow ${workflowId}, stage ${stage}`);
        await this.updateJobMetadata(workflowId, stage, JOB_STATUS.PROCESSING)
      }
      
      let result
      
      console.log(`🎯 Switching on jobType: ${jobType}`);
      
      switch (jobType) {
        case JOB_TYPES.AI_PARSE:
          console.log(`🤖 Calling processAIParsing...`);
          result = await this.processAIParsing(job)
          console.log(`🤖 processAIParsing completed:`, result);
          break
          
        case JOB_TYPES.DATABASE_SAVE:
          console.log(`💾 Calling processDatabaseSave...`);
          result = await this.processDatabaseSave(job)
          break
          
        case JOB_TYPES.SHOPIFY_SYNC:
          console.log(`🛒 Calling processShopifySync...`);
          result = await this.processShopifySync(job)
          break
          
        case JOB_TYPES.STATUS_UPDATE:
          console.log(`📊 Calling processStatusUpdate...`);
          result = await this.processStatusUpdate(job)
          break
          
        default:
          throw new Error(`Unknown job type: ${jobType}`)
      }
      
      console.log(`✅ Job processing completed, triggering next stage...`);
      
      // Trigger next stage in workflow
      if (workflowId && stage) {
        await this.triggerNextStage(workflowId, stage, result)
      }
      
      console.log(`🎉 processJob completed successfully for ${jobType}`);
      return result
      
    } catch (error) {
      console.error(`❌ Job processing failed for ${jobType}:`, error)
      console.error(`📋 Full error stack:`, error.stack);
      
      // Update workflow status to failed
      if (workflowId) {
        await this.updateWorkflowStatus(workflowId, WORKFLOW_STAGES.FAILED, {
          failedStage: stage,
          error: error.message,
          timestamp: new Date().toISOString()
        })
      }
      
      throw error
    }
  }

  /**
   * Process AI Parsing stage
   */
  async processAIParsing(job) {
    console.log('🤖 processAIParsing - Full job.data:', JSON.stringify(job.data, null, 2));
    
    // Extract data from nested structure
    const { workflowId, data } = job.data;
    const { fileName, fileBuffer, parsedContent, uploadId, mimeType, options } = data;
    
    console.log('🤖 Extracted data:', { 
      workflowId, 
      fileName, 
      hasFileBuffer: !!fileBuffer, 
      hasParsedContent: !!parsedContent,
      uploadId,
      mimeType,
      options 
    });
    
    // Use parsedContent if available, otherwise convert fileBuffer, otherwise download from storage
    let contentForProcessing;
    
    if (parsedContent) {
      console.log('📋 Using parsedContent from job data');
      contentForProcessing = parsedContent;
    } else if (fileBuffer) {
      console.log('📋 Converting fileBuffer to content');
      contentForProcessing = Buffer.from(fileBuffer.data).toString();
    } else {
      // Need to download file content from storage
      console.log('📥 Downloading file content from storage...');
      try {
        // Import storage service dynamically
        const { storageService } = await import('./storageService.js');
        const { db } = await import('./db.js');
        
        // Get upload record to find file URL
        const upload = await db.client.upload.findUnique({
          where: { id: uploadId },
          select: { fileUrl: true }
        });
        
        if (!upload || !upload.fileUrl) {
          throw new Error(`No file URL found for upload ${uploadId}`);
        }
        
        console.log('📁 File URL found:', upload.fileUrl);
        
        // Download file content
        const fileResult = await storageService.downloadFile(upload.fileUrl);
        if (!fileResult.success) {
          throw new Error(`Failed to download file: ${fileResult.error}`);
        }
        
        contentForProcessing = fileResult.content;
        console.log('� Downloaded file content, length:', contentForProcessing.length);
        
      } catch (error) {
        console.error('❌ Failed to download file content:', error);
        throw new Error(`Failed to retrieve file content: ${error.message}`);
      }
    }
    
    if (!contentForProcessing) {
      throw new Error('No file content provided for AI parsing');
    }
    
    console.log('📝 Content length:', contentForProcessing.length);
    
    job.progress(10)
    
    try {
      // Determine which parsing service to use based on file type
      const fileExtension = fileName.split('.').pop().toLowerCase();
      console.log(`📄 File extension: ${fileExtension}`);
      
      let result;
      
      if (['csv', 'xlsx', 'xls'].includes(fileExtension)) {
        // Use file parsing service for structured files
        console.log('📊 Using file parsing service for structured file');
        const { fileParsingService } = await import('./fileParsingService.js');
        const mimeType = fileExtension === 'csv' ? 'text/csv' : 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet';
        const parsingResult = await fileParsingService.parseFile(contentForProcessing, mimeType, fileName);
        
        result = {
          extractedData: parsingResult.data,
          confidence: parsingResult.confidence || 0.9,
          requiresReview: false,
          merchantMessage: 'File parsed successfully using structured parsing',
          qualityAssessment: { method: 'structured-parsing' },
          timestamp: new Date().toISOString()
        };
      } else {
        // Use enhanced AI service for images/PDFs
        console.log('🤖 Using AI service for image/PDF processing');
        const aiResult = await enhancedAIService.parseDocument(contentForProcessing, workflowId, {
          confidenceThreshold: options?.confidenceThreshold || 0.8
        });

        result = {
          extractedData: aiResult.extractedData,
          confidence: aiResult.confidence,
          requiresReview: aiResult.handlingResult.requiresReview || false,
          merchantMessage: aiResult.handlingResult.merchantMessage,
          qualityAssessment: aiResult.qualityAssessment,
          timestamp: new Date().toISOString()
        };
      }

      job.progress(80);
      
      // Validate result
      if (!result.extractedData) {
        throw new Error('No data extracted from file');
      }

      job.progress(100);
      return result;

    } catch (error) {
      console.error(`❌ AI parsing failed for workflow ${workflowId}:`, error)
      
      // Handle critical error through error handling service
      if (workflowId) {
        await errorHandlingService.handleCriticalError(workflowId, 'ai_parsing', error)
      }
      
      throw error
    }
  }

  /**
   * Process Database Save stage
   */
  async processDatabaseSave(job) {
    const { workflowId, data } = job.data
    const { extractedData, uploadId, merchantId } = data
    
    job.progress(20)
    
    try {
      const dbService = new DatabasePersistenceService()
      const savedPO = await dbService.savePurchaseOrder(
        extractedData,
        uploadId,
        merchantId
      )
      
      // Update workflow status
      if (workflowId) {
        await errorHandlingService.updateWorkflowStatus(workflowId, 'database_save_completed', {
          reason: 'purchase_order_saved',
          purchaseOrderId: savedPO.id,
          merchantMessage: '✅ Data saved successfully'
        })
      }
      
      job.progress(100)
      
      return {
        purchaseOrderId: savedPO.id,
        status: savedPO.status,
        timestamp: new Date().toISOString()
      }
    } catch (error) {
      console.error(`❌ Database save failed for workflow ${workflowId}:`, error)
      
      // Handle database error through error handling service
      if (workflowId) {
        await errorHandlingService.handleCriticalError(workflowId, 'database', error)
      }
      
      throw error
    }
  }

  /**
   * Process Shopify Sync stage
   */
  async processShopifySync(job) {
    const { workflowId, data } = job.data
    const { purchaseOrderId, attemptNumber = 1 } = data
    
    job.progress(10)
    
    try {
      // Check if PO is approved for sync
      const po = await db.purchaseOrder.findUnique({
        where: { id: purchaseOrderId },
        include: { items: true }
      })
      
      if (!po) {
        throw new Error(`Purchase Order ${purchaseOrderId} not found`)
      }
      
      if (po.status !== 'approved' && po.status !== 'review_needed') {
        console.log(`⏭️ Skipping Shopify sync for PO ${purchaseOrderId} - status: ${po.status}`)
        return {
          skipped: true,
          reason: `PO status is ${po.status}, not approved`,
          merchantMessage: '⏭️ Sync skipped - Purchase order not approved',
          timestamp: new Date().toISOString()
        }
      }
      
      job.progress(30)
      
      // Use enhanced Shopify service with error handling
      const syncResult = await enhancedShopifyService.syncPurchaseOrder(po, workflowId, attemptNumber)
      
      job.progress(80)
      
      if (syncResult.success) {
        if (workflowId) {
          await errorHandlingService.updateWorkflowStatus(workflowId, 'completed', {
            reason: 'shopify_sync_successful',
            merchantMessage: syncResult.merchantMessage || MERCHANT_MESSAGES.SYNC_SUCCESS,
            syncResults: syncResult.results
          })
        }
        
        job.progress(100)
        
        return {
          syncedToShopify: true,
          shopifyResult: syncResult,
          merchantMessage: syncResult.merchantMessage || MERCHANT_MESSAGES.SYNC_SUCCESS,
          timestamp: new Date().toISOString()
        }
      } else if (syncResult.partial) {
        if (workflowId) {
          await errorHandlingService.updateWorkflowStatus(workflowId, 'completed_with_warnings', {
            reason: 'shopify_sync_partial',
            merchantMessage: syncResult.merchantMessage || MERCHANT_MESSAGES.PARTIAL_SYNC,
            syncResults: syncResult.results,
            errors: syncResult.errors
          })
        }
        
        job.progress(100)
        
        return {
          syncedToShopify: true,
          partial: true,
          shopifyResult: syncResult,
          merchantMessage: syncResult.merchantMessage || MERCHANT_MESSAGES.PARTIAL_SYNC,
          timestamp: new Date().toISOString()
        }
      } else {
        // Sync failed - error handling service will handle retry logic
        throw new Error(`Shopify sync failed: ${syncResult.summary || 'Unknown error'}`)
      }

    } catch (error) {
      console.error(`❌ Shopify sync failed for workflow ${workflowId}:`, error)
      
      // Handle Shopify sync error through error handling service
      if (workflowId) {
        const handlingResult = await errorHandlingService.handleShopifySyncError(workflowId, error, attemptNumber)
        
        if (handlingResult.shouldRetry) {
          // Schedule retry
          setTimeout(async () => {
            const shopifyQueue = this.queues.get(JOB_TYPES.SHOPIFY_SYNC)
            if (shopifyQueue) {
              const retryJob = await shopifyQueue.add({
                workflowId,
                stage: WORKFLOW_STAGES.SHOPIFY_SYNC,
                data: {
                  ...data,
                  attemptNumber: handlingResult.attemptNumber
                }
              })
              
              console.log(`🔄 Scheduled Shopify sync retry for workflow ${workflowId} (attempt ${handlingResult.attemptNumber})`)
            }
          }, handlingResult.retryDelay)
          
          return {
            retrying: true,
            attemptNumber: handlingResult.attemptNumber,
            merchantMessage: handlingResult.merchantMessage,
            nextRetryAt: new Date(Date.now() + handlingResult.retryDelay).toISOString()
          }
        }
      }
      
      throw error
    }
  }

  /**
   * Process Status Update stage
   */
  async processStatusUpdate(job) {
    const { workflowId, data } = job.data
    const { purchaseOrderId, finalStatus } = data
    
    job.progress(50)
    
    try {
      const updatedPO = await db.purchaseOrder.update({
        where: { id: purchaseOrderId },
        data: {
          status: finalStatus,
          updatedAt: new Date()
        }
      })
      
      job.progress(100)
      
      return {
        updatedStatus: finalStatus,
        purchaseOrderId,
        timestamp: new Date().toISOString()
      }
    } catch (error) {
      console.error(`❌ Status update failed for workflow ${workflowId}:`, error)
      throw error
    }
  }

  /**
   * Start a new workflow
   */
  async startWorkflow(workflowData) {
    const workflowId = `workflow_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
    
    console.log(`🚀 Starting workflow ${workflowId}`)
    
    // Initialize workflow metadata
    await this.initializeWorkflowMetadata(workflowId, workflowData)
    
    // Start with AI parsing stage
    await this.addJobToQueue(JOB_TYPES.AI_PARSE, {
      workflowId,
      stage: WORKFLOW_STAGES.AI_PARSING,
      data: workflowData
    })
    
    this.stats.workflows.started++
    
    return workflowId
  }

  /**
   * Trigger next stage in workflow
   */
  async triggerNextStage(workflowId, currentStage, stageResult) {
    console.log(`🔄 Triggering next stage after ${currentStage} for workflow ${workflowId}`)
    
    const stageOrder = [
      WORKFLOW_STAGES.AI_PARSING,
      WORKFLOW_STAGES.DATABASE_SAVE,
      WORKFLOW_STAGES.SHOPIFY_SYNC,
      WORKFLOW_STAGES.STATUS_UPDATE,
      WORKFLOW_STAGES.COMPLETED
    ]
    
    const currentIndex = stageOrder.indexOf(currentStage)
    const nextStage = stageOrder[currentIndex + 1]
    
    if (!nextStage || nextStage === WORKFLOW_STAGES.COMPLETED) {
      // Workflow completed
      await this.updateWorkflowStatus(workflowId, WORKFLOW_STAGES.COMPLETED, {
        completedAt: new Date().toISOString(),
        finalResult: stageResult
      })
      
      this.stats.workflows.completed++
      console.log(`🎉 Workflow ${workflowId} completed successfully`)
      return
    }
    
    // Prepare data for next stage
    let nextStageData = {}
    
    switch (nextStage) {
      case WORKFLOW_STAGES.DATABASE_SAVE:
        nextStageData = {
          extractedData: stageResult.extractedData,
          uploadId: await this.getWorkflowMetadata(workflowId, 'uploadId'),
          merchantId: await this.getWorkflowMetadata(workflowId, 'merchantId')
        }
        break
        
      case WORKFLOW_STAGES.SHOPIFY_SYNC:
        nextStageData = {
          purchaseOrderId: stageResult.purchaseOrderId
        }
        break
        
      case WORKFLOW_STAGES.STATUS_UPDATE:
        const wasShopifySynced = stageResult.syncedToShopify && !stageResult.skipped
        nextStageData = {
          purchaseOrderId: await this.getWorkflowMetadata(workflowId, 'purchaseOrderId') || stageResult.purchaseOrderId,
          finalStatus: wasShopifySynced ? 'completed' : 'review_needed'
        }
        break
    }
    
    // Add next stage job to appropriate queue
    await this.addJobToQueue(this.getJobTypeForStage(nextStage), {
      workflowId,
      stage: nextStage,
      data: nextStageData
    })
  }

  /**
   * Add job to appropriate queue
   */
  async addJobToQueue(jobType, jobData) {
    if (!this.isInitialized) {
      await this.initialize()
    }
    
    const queue = this.queues.get(jobType)
    if (!queue) {
      throw new Error(`Queue not found for job type: ${jobType}`)
    }
    
    console.log(`📋 Adding job to ${jobType} queue...`);
    const job = await queue.add(jobData)
    
    console.log(`✅ Added ${jobType} job ${job.id} to queue`);
    
    // Check job state after 1 second for debugging
    setTimeout(async () => {
      try {
        const jobState = await job.getState();
        console.log(`📍 Job ${job.id} state after 1s: ${jobState}`);
      } catch (error) {
        console.log(`📍 Could not check job ${job.id} state:`, error.message);
      }
    }, 1000);
    
    return job
  }

  /**
   * Get job type for workflow stage
   */
  getJobTypeForStage(stage) {
    const stageToJobType = {
      [WORKFLOW_STAGES.AI_PARSING]: JOB_TYPES.AI_PARSE,
      [WORKFLOW_STAGES.DATABASE_SAVE]: JOB_TYPES.DATABASE_SAVE,
      [WORKFLOW_STAGES.SHOPIFY_SYNC]: JOB_TYPES.SHOPIFY_SYNC,
      [WORKFLOW_STAGES.STATUS_UPDATE]: JOB_TYPES.STATUS_UPDATE
    }
    
    return stageToJobType[stage]
  }

  /**
   * Initialize workflow metadata in Redis
   */
  async initializeWorkflowMetadata(workflowId, workflowData) {
    const metadata = {
      workflowId,
      status: WORKFLOW_STAGES.AI_PARSING,
      startedAt: new Date().toISOString(),
      stages: {
        [WORKFLOW_STAGES.AI_PARSING]: { status: JOB_STATUS.PENDING },
        [WORKFLOW_STAGES.DATABASE_SAVE]: { status: JOB_STATUS.PENDING },
        [WORKFLOW_STAGES.SHOPIFY_SYNC]: { status: JOB_STATUS.PENDING },
        [WORKFLOW_STAGES.STATUS_UPDATE]: { status: JOB_STATUS.PENDING }
      },
      data: workflowData
    }
    
    await this.redisManager.set(`workflow:${workflowId}`, JSON.stringify(metadata), 86400) // 24 hour TTL
  }

  /**
   * Update workflow status
   */
  async updateWorkflowStatus(workflowId, status, additionalData = {}) {
    try {
      const metadataKey = `workflow:${workflowId}`
      const existing = await this.redisManager.get(metadataKey)
      
      if (existing) {
        const metadata = JSON.parse(existing)
        metadata.status = status
        metadata.updatedAt = new Date().toISOString()
        
        Object.assign(metadata, additionalData)
        
        await this.redisManager.set(metadataKey, JSON.stringify(metadata), 86400)
      }
    } catch (error) {
      console.warn(`⚠️ Failed to update workflow status for ${workflowId} - ${error.message}`);
      // Don't throw - allow job processing to continue even if status update fails
    }
  }

  /**
   * Update job metadata for specific stage
   */
  async updateJobMetadata(workflowId, stage, status, additionalData = {}) {
    try {
      const metadataKey = `workflow:${workflowId}`
      const existing = await this.redisManager.get(metadataKey)
      
      if (existing) {
        const metadata = JSON.parse(existing)
        
        if (!metadata.stages[stage]) {
          metadata.stages[stage] = {}
        }
        
        metadata.stages[stage].status = status
        metadata.stages[stage].updatedAt = new Date().toISOString()
        
        Object.assign(metadata.stages[stage], additionalData)
        
        await this.redisManager.set(metadataKey, JSON.stringify(metadata), 86400)
      }
    } catch (error) {
      console.warn(`⚠️ Failed to update job metadata for ${workflowId}:${stage} - ${error.message}`);
      // Don't throw - allow job processing to continue even if metadata update fails
    }
  }

  /**
   * Get workflow metadata
   */
  async getWorkflowMetadata(workflowId, key = null) {
    try {
      const metadataKey = `workflow:${workflowId}`
      const existing = await this.redisManager.get(metadataKey)
      
      if (!existing) return null
      
      const metadata = JSON.parse(existing)
      
      return key ? metadata.data?.[key] : metadata
    } catch (error) {
      console.warn(`⚠️ Failed to get workflow metadata for ${workflowId} - ${error.message}`);
      return null
    }
  }

  /**
   * Get workflow status
   */
  async getWorkflowStatus(workflowId) {
    return await this.getWorkflowMetadata(workflowId)
  }

  /**
   * Get orchestrator statistics
   */
  async getStatistics() {
    const queueStats = []
    
    for (const [type, queue] of this.queues.entries()) {
      try {
        const waiting = await queue.getWaiting()
        const active = await queue.getActive()
        const completed = await queue.getCompleted()
        const failed = await queue.getFailed()
        
        queueStats.push({
          type,
          waiting: waiting.length,
          active: active.length,
          completed: completed.length,
          failed: failed.length
        })
      } catch (error) {
        queueStats.push({
          type,
          waiting: 0,
          active: 0,
          completed: 0,
          failed: 0,
          error: error.message
        })
      }
    }
    
    return {
      ...this.stats,
      queues: queueStats
    }
  }

  /**
   * Cleanup completed workflows (housekeeping)
   */
  async cleanup(olderThanHours = 24) {
    const cutoffTime = new Date(Date.now() - (olderThanHours * 60 * 60 * 1000))
    
    // This would require scanning Redis keys and removing old workflows
    // Implementation depends on Redis key management strategy
    console.log(`🧹 Cleaning up workflows older than ${olderThanHours} hours`)
  }

  /**
   * Graceful shutdown
   */
  async shutdown() {
    console.log('🛑 Shutting down Workflow Orchestrator...')
    
    const shutdownPromises = Array.from(this.queues.values()).map(queue => 
      queue.close()
    )
    
    await Promise.all(shutdownPromises)
    
    console.log('✅ Workflow Orchestrator shutdown complete')
  }
}

// Create singleton instance
export const workflowOrchestrator = new WorkflowOrchestrator()