/**
 * Workflow Orchestration Service
 * 
 * Manages multi-stage background job processing with Redis:
 * 1. File Upload ‚Üí AI Parse
 * 2. AI Parse ‚Üí Database Save  
 * 3. Database Save ‚Üí Shopify Sync (if approved)
 * 4. Shopify Sync ‚Üí Update PO Status ‚Üí Complete
 * 
 * Each stage is tracked with comprehensive metadata for monitoring and debugging.
 */
      // This matches our successful test pattern: queue.process(concurrency, processor)
      console.log(`‚úÖ Registering processor for ${jobType}...`);
      queue.process(concurrency, processorFunction);
      
      // Explicitly resume the queue to ensure it's not paused
      await queue.resume();
      console.log(`‚ñ∂Ô∏è Queue ${jobType} resumed and ready`);istration (no job name)
      // This matches the pattern from our successful tests
      console.log(`üìù Registering default processor for ${jobType} with concurrency ${concurrency}...`);
      
      // Create processor function with extensive logging
      const processorFunction = async (job) => {
        console.log(`üö® === PROCESSOR FUNCTION CALLED ===`);
        console.log(`üéØ Queue: ${jobType}, Job ID: ${job.id}`);
        console.log(`üìã Job data:`, JSON.stringify(job.data, null, 2));
        
        try {
          const result = await this.processJob(job, jobType);
          console.log(`‚úÖ Job ${job.id} processed successfully:`, result);
          return result;
        } catch (error) {
          console.error(`üí• Error processing job ${job.id}:`, error);
          throw error;
        }
      };
      
      console.log(`üî® About to call queue.process() for ${jobType}...`);
      
      // Register processor with concurrency only (no job name)
      queue.process(concurrency, processorFunction);
      
      console.log(`‚úÖ queue.process() called successfully for ${jobType}`);
      
      // Test the processor registration immediately
      setTimeout(async () => {
        console.log(`üß™ Testing processor registration for ${jobType}...`);
        try {
          const testJob = await queue.add({ 
            test: true, 
            jobType: jobType,
            timestamp: Date.now(),
            message: 'Immediate test job to verify processor' 
          });
          console.log(`üìã Test job ${testJob.id} added to ${jobType} queue for immediate processing test`);
        } catch (error) {
          console.error(`‚ùå Failed to add test job to ${jobType}:`, error);
        }
      }, 2000);
      
      // Explicitly resume the queue to ensure it's not paused
      await queue.resume();
      console.log(`‚ñ∂Ô∏è Queue ${jobType} resumed and ready`);
      
    } catch (error) {
      console.error(`‚ùå Failed to register processor for ${jobType}:`, error);
      console.error(`üìã Full error stack:`, error.stack);
      throw error;
    }gister processor using the same pattern as our successful tests
      console.log(`ÔøΩ Registering processor for ${jobType}...`);
      
      // Register for the specific job type name
      queue.process(jobType, processorFunction);
      
      // Then resume the queue to ensure it's ready
      await queue.resume();
      console.log(`‚ñ∂Ô∏è Queue ${jobType} resumed and ready`);ify Sync ‚Üí Update PO Status ‚Üí Complete
 * 
 * Each stage is tracked with comprehensive metadata for monitoring and debugging.
 */

import Bull from 'bull'
import redisManager from './redisManager.js'
import { getRedisConfig } from '../config/redis.production.js'
import { enhancedAIService } from './enhancedAIService.js'
import { enhancedShopifyService } from './enhancedShopifyService.js'
import { errorHandlingService, MERCHANT_MESSAGES } from './errorHandlingService.js'
import { DatabasePersistenceService } from './databasePersistenceService.js'
import { db } from './db.js'
import { processorRegistrationService } from './processorRegistrationService.js'

// Workflow Stage Definitions
export const WORKFLOW_STAGES = {
  FILE_UPLOAD: 'file_upload',
  AI_PARSING: 'ai_parsing', 
  DATABASE_SAVE: 'database_save',
  SHOPIFY_SYNC: 'shopify_sync',
  STATUS_UPDATE: 'status_update',
  COMPLETED: 'completed',
  FAILED: 'failed'
}

// Job Types for different queues
export const JOB_TYPES = {
  AI_PARSE: 'ai_parse',
  DATABASE_SAVE: 'database_save',
  SHOPIFY_SYNC: 'shopify_sync',
  STATUS_UPDATE: 'status_update'
}

// Job Status Tracking
export const JOB_STATUS = {
  PENDING: 'pending',
  PROCESSING: 'processing',
  COMPLETED: 'completed',
  FAILED: 'failed',
  RETRYING: 'retrying',
  CANCELLED: 'cancelled'
}

export class WorkflowOrchestrator {
  constructor() {
    this.queues = new Map()
    this.redisManager = redisManager // Use singleton instance
    this.isInitialized = false
    
    // Workflow configuration
    this.config = {
      retryAttempts: 3,
      retryDelay: 5000, // 5 seconds
      jobTimeout: 300000, // 5 minutes default
      concurrency: {
        ai_parse: 2,
        database_save: 5,
        shopify_sync: 3,
        status_update: 10
      }
    }
    
    // Statistics tracking
    this.stats = {
      workflows: {
        started: 0,
        completed: 0,
        failed: 0
      },
      stages: {
        [WORKFLOW_STAGES.AI_PARSING]: { processed: 0, failed: 0 },
        [WORKFLOW_STAGES.DATABASE_SAVE]: { processed: 0, failed: 0 },
        [WORKFLOW_STAGES.SHOPIFY_SYNC]: { processed: 0, failed: 0 },
        [WORKFLOW_STAGES.STATUS_UPDATE]: { processed: 0, failed: 0 }
      }
    }
  }

  /**
   * Initialize all workflow queues
   */
  async initialize() {
    if (this.isInitialized) return

    try {
      console.log('üöÄ Initializing Workflow Orchestrator...')
      
      await this.redisManager.waitForConnection(15000)
      
      const environment = process.env.NODE_ENV || 'development'
      const redisConfig = getRedisConfig(environment)
      const connectionOptions = redisConfig.connection

      // Create specialized queues for each workflow stage
      const queueConfigs = [
        {
          name: 'ai-parsing',
          type: JOB_TYPES.AI_PARSE,
          concurrency: this.config.concurrency.ai_parse,
          timeout: 600000 // 10 minutes for AI processing
        },
        {
          name: 'database-save',
          type: JOB_TYPES.DATABASE_SAVE,
          concurrency: this.config.concurrency.database_save,
          timeout: 60000 // 1 minute for DB operations
        },
        {
          name: 'shopify-sync',
          type: JOB_TYPES.SHOPIFY_SYNC,
          concurrency: this.config.concurrency.shopify_sync,
          timeout: 300000 // 5 minutes for Shopify operations
        },
        {
          name: 'status-update',
          type: JOB_TYPES.STATUS_UPDATE,
          concurrency: this.config.concurrency.status_update,
          timeout: 30000 // 30 seconds for status updates
        }
      ]

      for (const queueConfig of queueConfigs) {
        const queue = new Bull(queueConfig.name, {
          redis: connectionOptions,
          defaultJobOptions: {
            attempts: this.config.retryAttempts,
            backoff: {
              type: 'exponential',
              delay: this.config.retryDelay
            },
            removeOnComplete: 100,
            removeOnFail: 50,
            timeout: queueConfig.timeout
          },
          settings: {
            stalledInterval: 30000,
            maxStalledCount: 1
          }
        })

        console.log(`üîß Created Bull queue: ${queueConfig.name}`)

        // Use permanent processor registration fix instead of corrupted setupQueueProcessor
        console.log(`üîß Using permanent processor registration fix for ${queueConfig.type}`)
        
        // Setup event handlers
        this.setupQueueEvents(queue, queueConfig.type)
        
        this.queues.set(queueConfig.type, queue)
        
        // Force queue to start processing if there are waiting jobs
        const waitingJobs = await queue.getWaiting()
        if (waitingJobs.length > 0) {
          console.log(`üöÄ Found ${waitingJobs.length} waiting jobs in ${queueConfig.name}, attempting to trigger processing...`)
          
          // In Bull v4, sometimes we need to explicitly wake up the queue
          try {
            // Small delay to ensure processor is fully registered
            await new Promise(resolve => setTimeout(resolve, 100));
            
            // Try to wake up the queue processing
            await queue.resume();
            
            // Check if any jobs moved to active after resume
            setTimeout(async () => {
              const activeAfterResume = await queue.getActive();
              console.log(`ÔøΩ After resume - Active jobs: ${activeAfterResume.length}`);
            }, 1000);
            
          } catch (error) {
            console.log(`‚ùå Queue wake-up failed:`, error.message)
          }
        }
        
        // Log queue state after setup
        const waiting = await queue.getWaiting()
        const active = await queue.getActive()
        const completed = await queue.getCompleted()
        const failed = await queue.getFailed()
        
        console.log(`üìä Queue ${queueConfig.name} state: waiting=${waiting.length}, active=${active.length}, completed=${completed.length}, failed=${failed.length}`)
        
        console.log(`‚úÖ Initialized ${queueConfig.name} queue (concurrency: ${queueConfig.concurrency})`)
      }

      // Initialize all processors using the permanent fix
      console.log('üöÄ Initializing processors with permanent fix...')
      await processorRegistrationService.initializeAllProcessors(this)

      this.isInitialized = true
      console.log('üéâ Workflow Orchestrator initialized successfully')
      
    } catch (error) {
      console.error('‚ùå Failed to initialize Workflow Orchestrator:', error)
      throw error
    }
  }

  /**
   * Setup queue processor for specific job type
   */
  async setupQueueProcessor(queue, jobType, concurrency) {
    console.log(`üîß Setting up processor for ${jobType} with concurrency ${concurrency}`);
    
    // Setup processor function with comprehensive error handling
    const processorFunction = async (job) => {
      try {
        console.log(`üéØ BULL PROCESSOR TRIGGERED for ${jobType} job ${job.id}`);
        console.log(`üìã Job data:`, JSON.stringify(job.data, null, 2));
        
        const result = await this.processJob(job, jobType);
        console.log(`‚úÖ Completed ${jobType} job ${job.id}:`, result);
        return result;
      } catch (error) {
        console.error(`üí• PROCESSOR ERROR in ${jobType} job ${job.id}:`, error);
        console.error(`üìã Error stack:`, error.stack);
        throw error; // Re-throw so Bull can handle it properly
      }
    };
    
    try {
      // Use the simplest processor registration (like our successful tests)
      console.log(`ÔøΩ Registering processor for ${jobType}...`);
      queue.process(jobType, processorFunction);
      
      // Explicitly resume the queue to ensure it's not paused
      await queue.resume();
      console.log(`‚ñ∂Ô∏è Queue ${jobType} resumed and ready`);
      
    } catch (error) {
      console.error(`‚ùå Failed to register processor for ${jobType}:`, error);
      throw error;
    }
    
    // Check for waiting jobs and log details
    try {
      const waitingJobs = await queue.getWaiting();
      console.log(`üîç Queue ${jobType} has ${waitingJobs.length} waiting jobs`);
      
      // If there are waiting jobs, log the first one for debugging
      if (waitingJobs.length > 0) {
        const firstJob = waitingJobs[0];
        console.log(`üî¨ First waiting job:`, {
          id: firstJob.id,
          name: firstJob.name,
          workflowId: firstJob.data?.workflowId,
          attempts: firstJob.attemptsMade
        });
      }
    } catch (error) {
      console.log(`üîç Queue ${jobType} status check failed:`, error.message);
    }
    
    console.log(`‚úÖ Initialized ${jobType} queue processor`);
  }

  /**
   * Setup event handlers for queue monitoring
   */
  setupQueueEvents(queue, jobType) {
    queue.on('completed', (job, result) => {
      console.log(`‚úÖ ${jobType} job ${job.id} completed`)
      this.updateJobMetadata(job.data.workflowId, job.data.stage, JOB_STATUS.COMPLETED, { result })
      this.stats.stages[job.data.stage].processed++
    })

    queue.on('failed', (job, err) => {
      console.error(`‚ùå ${jobType} job ${job.id} failed:`, err.message)
      this.updateJobMetadata(job.data.workflowId, job.data.stage, JOB_STATUS.FAILED, { error: err.message })
      this.stats.stages[job.data.stage].failed++
    })

    queue.on('stalled', (job) => {
      console.warn(`‚ö†Ô∏è ${jobType} job ${job.id} stalled`)
      this.updateJobMetadata(job.data.workflowId, job.data.stage, JOB_STATUS.RETRYING)
    })

    queue.on('progress', (job, progress) => {
      console.log(`üìä ${jobType} job ${job.id} progress: ${progress}%`)
      this.updateJobMetadata(job.data.workflowId, job.data.stage, JOB_STATUS.PROCESSING, { progress })
    })
  }

  /**
   * Process individual job based on type
   */
  async processJob(job, jobType) {
    console.log(`üî¨ processJob called: jobType=${jobType}, jobId=${job.id}`);
    
    const { workflowId, stage, data } = job.data
    
    console.log(`üîÑ Processing ${jobType} job for workflow ${workflowId}`)
    
    try {
      // Update job status to processing
      console.log(`üìù Updating job metadata for workflow ${workflowId}, stage ${stage}`);
      await this.updateJobMetadata(workflowId, stage, JOB_STATUS.PROCESSING)
      
      let result
      
      console.log(`üéØ Switching on jobType: ${jobType}`);
      
      switch (jobType) {
        case JOB_TYPES.AI_PARSE:
          console.log(`ü§ñ Calling processAIParsing...`);
          result = await this.processAIParsing(job)
          console.log(`ü§ñ processAIParsing completed:`, result);
          break
          
        case JOB_TYPES.DATABASE_SAVE:
          console.log(`üíæ Calling processDatabaseSave...`);
          result = await this.processDatabaseSave(job)
          break
          
        case JOB_TYPES.SHOPIFY_SYNC:
          console.log(`üõí Calling processShopifySync...`);
          result = await this.processShopifySync(job)
          break
          
        case JOB_TYPES.STATUS_UPDATE:
          console.log(`üìä Calling processStatusUpdate...`);
          result = await this.processStatusUpdate(job)
          break
          
        default:
          throw new Error(`Unknown job type: ${jobType}`)
      }
      
      console.log(`‚úÖ Job processing completed, triggering next stage...`);
      
      // Trigger next stage in workflow
      await this.triggerNextStage(workflowId, stage, result)
      
      console.log(`üéâ processJob completed successfully for ${jobType}`);
      return result
      
    } catch (error) {
      console.error(`‚ùå Job processing failed for ${jobType}:`, error)
      console.error(`üìã Full error stack:`, error.stack);
      
      // Update workflow status to failed
      await this.updateWorkflowStatus(workflowId, WORKFLOW_STAGES.FAILED, {
        failedStage: stage,
        error: error.message,
        timestamp: new Date().toISOString()
      })
      
      throw error
    }
  }

  /**
   * Process AI Parsing stage
   */
  async processAIParsing(job) {
    console.log('ü§ñ processAIParsing - Full job.data:', JSON.stringify(job.data, null, 2));
    
    // Extract data from nested structure
    const { workflowId, data } = job.data;
    const { fileName, fileBuffer, parsedContent, options } = data;
    
    console.log('ü§ñ Extracted data:', { 
      workflowId, 
      fileName, 
      hasFileBuffer: !!fileBuffer, 
      hasParsedContent: !!parsedContent, 
      options 
    });
    
    // Use parsedContent if available, otherwise convert fileBuffer
    let contentForProcessing;
    if (parsedContent) {
      console.log('üìã Using parsedContent from job data');
      contentForProcessing = parsedContent;
    } else if (fileBuffer) {
      console.log('üìã Converting fileBuffer to content');
      contentForProcessing = Buffer.from(fileBuffer.data).toString();
    } else {
      throw new Error('No file content provided for AI parsing');
    }
    
    console.log('üìù Content length:', contentForProcessing ? contentForProcessing.length : 0);
    
    job.progress(10)
    
    try {
      // Determine which parsing service to use based on file type
      const fileExtension = fileName.split('.').pop().toLowerCase();
      console.log(`üìÑ File extension: ${fileExtension}`);
      
      let result;
      
      if (['csv', 'xlsx', 'xls'].includes(fileExtension)) {
        // Use file parsing service for structured files
        console.log('üìä Using file parsing service for structured file');
        const { fileParsingService } = await import('./fileParsingService.js');
        const mimeType = fileExtension === 'csv' ? 'text/csv' : 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet';
        const parsingResult = await fileParsingService.parseFile(contentForProcessing, mimeType, fileName);
        
        result = {
          extractedData: parsingResult.data,
          confidence: parsingResult.confidence || 0.9,
          requiresReview: false,
          merchantMessage: 'File parsed successfully using structured parsing',
          qualityAssessment: { method: 'structured-parsing' },
          timestamp: new Date().toISOString()
        };
      } else {
        // Use enhanced AI service for images/PDFs
        console.log('ü§ñ Using AI service for image/PDF processing');
        const aiResult = await enhancedAIService.parseDocument(contentForProcessing, workflowId, {
          confidenceThreshold: options?.confidenceThreshold || 0.8
        });

        result = {
          extractedData: aiResult.extractedData,
          confidence: aiResult.confidence,
          requiresReview: aiResult.handlingResult.requiresReview || false,
          merchantMessage: aiResult.handlingResult.merchantMessage,
          qualityAssessment: aiResult.qualityAssessment,
          timestamp: new Date().toISOString()
        };
      }

      job.progress(80);
      
      // Validate result
      if (!result.extractedData) {
        throw new Error('No data extracted from file');
      }

      job.progress(100);
      return result;

    } catch (error) {
      console.error(`‚ùå AI parsing failed for workflow ${workflowId}:`, error)
      
      // Handle critical error through error handling service
      await errorHandlingService.handleCriticalError(workflowId, 'ai_parsing', error)
      
      throw error
    }
  }

  /**
   * Process Database Save stage
   */
  async processDatabaseSave(job) {
    const { extractedData, uploadId, merchantId, workflowId } = job.data
    
    job.progress(20)
    
    try {
      const dbService = new DatabasePersistenceService()
      const savedPO = await dbService.savePurchaseOrder(
        extractedData,
        uploadId,
        merchantId
      )
      
      // Update workflow status
      await errorHandlingService.updateWorkflowStatus(workflowId, 'database_save_completed', {
        reason: 'purchase_order_saved',
        purchaseOrderId: savedPO.id,
        merchantMessage: '‚úÖ Data saved successfully'
      })
      
      job.progress(100)
      
      return {
        purchaseOrderId: savedPO.id,
        status: savedPO.status,
        timestamp: new Date().toISOString()
      }
    } catch (error) {
      console.error(`‚ùå Database save failed for workflow ${workflowId}:`, error)
      
      // Handle database error through error handling service
      await errorHandlingService.handleCriticalError(workflowId, 'database', error)
      
      throw error
    }
  }

  /**
   * Process Shopify Sync stage
   */
  async processShopifySync(job) {
    const { purchaseOrderId, workflowId, attemptNumber = 1 } = job.data
    
    job.progress(10)
    
    try {
      // Check if PO is approved for sync
      const po = await db.purchaseOrder.findUnique({
        where: { id: purchaseOrderId },
        include: { items: true }
      })
      
      if (!po) {
        throw new Error(`Purchase Order ${purchaseOrderId} not found`)
      }
      
      if (po.status !== 'approved' && po.status !== 'review_needed') {
        console.log(`‚è≠Ô∏è Skipping Shopify sync for PO ${purchaseOrderId} - status: ${po.status}`)
        return {
          skipped: true,
          reason: `PO status is ${po.status}, not approved`,
          merchantMessage: '‚è≠Ô∏è Sync skipped - Purchase order not approved',
          timestamp: new Date().toISOString()
        }
      }
      
      job.progress(30)
      
      // Use enhanced Shopify service with error handling
      const syncResult = await enhancedShopifyService.syncPurchaseOrder(po, workflowId, attemptNumber)
      
      job.progress(80)
      
      if (syncResult.success) {
        await errorHandlingService.updateWorkflowStatus(workflowId, 'completed', {
          reason: 'shopify_sync_successful',
          merchantMessage: syncResult.merchantMessage || MERCHANT_MESSAGES.SYNC_SUCCESS,
          syncResults: syncResult.results
        })
        
        job.progress(100)
        
        return {
          syncedToShopify: true,
          shopifyResult: syncResult,
          merchantMessage: syncResult.merchantMessage || MERCHANT_MESSAGES.SYNC_SUCCESS,
          timestamp: new Date().toISOString()
        }
      } else if (syncResult.partial) {
        await errorHandlingService.updateWorkflowStatus(workflowId, 'completed_with_warnings', {
          reason: 'shopify_sync_partial',
          merchantMessage: syncResult.merchantMessage || MERCHANT_MESSAGES.PARTIAL_SYNC,
          syncResults: syncResult.results,
          errors: syncResult.errors
        })
        
        job.progress(100)
        
        return {
          syncedToShopify: true,
          partial: true,
          shopifyResult: syncResult,
          merchantMessage: syncResult.merchantMessage || MERCHANT_MESSAGES.PARTIAL_SYNC,
          timestamp: new Date().toISOString()
        }
      } else {
        // Sync failed - error handling service will handle retry logic
        throw new Error(`Shopify sync failed: ${syncResult.summary || 'Unknown error'}`)
      }

    } catch (error) {
      console.error(`‚ùå Shopify sync failed for workflow ${workflowId}:`, error)
      
      // Handle Shopify sync error through error handling service
      const handlingResult = await errorHandlingService.handleShopifySyncError(workflowId, error, attemptNumber)
      
      if (handlingResult.shouldRetry) {
        // Schedule retry
        setTimeout(async () => {
          const retryJob = await this.queues.shopifySync.add('sync-to-shopify', {
            ...job.data,
            attemptNumber: handlingResult.attemptNumber
          }, this.getJobOptions('shopify_sync'))
          
          console.log(`üîÑ Scheduled Shopify sync retry for workflow ${workflowId} (attempt ${handlingResult.attemptNumber})`)
        }, handlingResult.retryDelay)
        
        return {
          retrying: true,
          attemptNumber: handlingResult.attemptNumber,
          merchantMessage: handlingResult.merchantMessage,
          nextRetryAt: new Date(Date.now() + handlingResult.retryDelay).toISOString()
        }
      }
      
      throw error
    }
  }

  /**
   * Process Status Update stage
   */
  async processStatusUpdate(job) {
    const { purchaseOrderId, finalStatus } = job.data.data
    
    job.progress(50)
    
    const updatedPO = await db.purchaseOrder.update({
      where: { id: purchaseOrderId },
      data: {
        status: finalStatus,
        updatedAt: new Date()
      }
    })
    
    job.progress(100)
    
    return {
      updatedStatus: finalStatus,
      purchaseOrderId,
      timestamp: new Date().toISOString()
    }
  }

  /**
   * Start a new workflow
   */
  async startWorkflow(workflowData) {
    const workflowId = `workflow_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
    
    console.log(`üöÄ Starting workflow ${workflowId}`)
    
    // Initialize workflow metadata
    await this.initializeWorkflowMetadata(workflowId, workflowData)
    
    // Start with AI parsing stage
    await this.addJobToQueue(JOB_TYPES.AI_PARSE, {
      workflowId,
      stage: WORKFLOW_STAGES.AI_PARSING,
      data: workflowData
    })
    
    this.stats.workflows.started++
    
    return workflowId
  }

  /**
   * Trigger next stage in workflow
   */
  async triggerNextStage(workflowId, currentStage, stageResult) {
    console.log(`üîÑ Triggering next stage after ${currentStage} for workflow ${workflowId}`)
    
    const stageOrder = [
      WORKFLOW_STAGES.AI_PARSING,
      WORKFLOW_STAGES.DATABASE_SAVE,
      WORKFLOW_STAGES.SHOPIFY_SYNC,
      WORKFLOW_STAGES.STATUS_UPDATE,
      WORKFLOW_STAGES.COMPLETED
    ]
    
    const currentIndex = stageOrder.indexOf(currentStage)
    const nextStage = stageOrder[currentIndex + 1]
    
    if (!nextStage || nextStage === WORKFLOW_STAGES.COMPLETED) {
      // Workflow completed
      await this.updateWorkflowStatus(workflowId, WORKFLOW_STAGES.COMPLETED, {
        completedAt: new Date().toISOString(),
        finalResult: stageResult
      })
      
      this.stats.workflows.completed++
      console.log(`üéâ Workflow ${workflowId} completed successfully`)
      return
    }
    
    // Prepare data for next stage
    let nextStageData = {}
    
    switch (nextStage) {
      case WORKFLOW_STAGES.DATABASE_SAVE:
        nextStageData = {
          extractedData: stageResult.extractedData,
          uploadId: await this.getWorkflowMetadata(workflowId, 'uploadId'),
          merchantId: await this.getWorkflowMetadata(workflowId, 'merchantId')
        }
        break
        
      case WORKFLOW_STAGES.SHOPIFY_SYNC:
        nextStageData = {
          purchaseOrderId: stageResult.purchaseOrderId
        }
        break
        
      case WORKFLOW_STAGES.STATUS_UPDATE:
        const wasShopifySynced = stageResult.syncedToShopify && !stageResult.skipped
        nextStageData = {
          purchaseOrderId: await this.getWorkflowMetadata(workflowId, 'purchaseOrderId') || stageResult.purchaseOrderId,
          finalStatus: wasShopifySynced ? 'completed' : 'review_needed'
        }
        break
    }
    
    // Add next stage job to appropriate queue
    await this.addJobToQueue(this.getJobTypeForStage(nextStage), {
      workflowId,
      stage: nextStage,
      data: nextStageData
    })
  }

  /**
   * Add job to appropriate queue
   */
  async addJobToQueue(jobType, jobData) {
    if (!this.isInitialized) {
      await this.initialize()
    }
    
    const queue = this.queues.get(jobType)
    if (!queue) {
      throw new Error(`Queue not found for job type: ${jobType}`)
    }
    
    // CRITICAL FIX: Add job without job name (use default)
    // This ensures it matches our default processor registration
    console.log(`üìã Adding job to ${jobType} queue...`);
    const job = await queue.add(jobData)
    
    console.log(`‚úÖ Added ${jobType} job ${job.id} to queue`);
    
    // Check job state after 1 second for debugging
    setTimeout(async () => {
      try {
        const jobState = await job.getState();
        console.log(`üìç Job ${job.id} state after 1s: ${jobState}`);
      } catch (error) {
        console.log(`üìç Could not check job ${job.id} state:`, error.message);
      }
    }, 1000);
    
    return job
  }

  /**
   * Get job type for workflow stage
   */
  getJobTypeForStage(stage) {
    const stageToJobType = {
      [WORKFLOW_STAGES.AI_PARSING]: JOB_TYPES.AI_PARSE,
      [WORKFLOW_STAGES.DATABASE_SAVE]: JOB_TYPES.DATABASE_SAVE,
      [WORKFLOW_STAGES.SHOPIFY_SYNC]: JOB_TYPES.SHOPIFY_SYNC,
      [WORKFLOW_STAGES.STATUS_UPDATE]: JOB_TYPES.STATUS_UPDATE
    }
    
    return stageToJobType[stage]
  }

  /**
   * Initialize workflow metadata in Redis
   */
  async initializeWorkflowMetadata(workflowId, workflowData) {
    const metadata = {
      workflowId,
      status: WORKFLOW_STAGES.AI_PARSING,
      startedAt: new Date().toISOString(),
      stages: {
        [WORKFLOW_STAGES.AI_PARSING]: { status: JOB_STATUS.PENDING },
        [WORKFLOW_STAGES.DATABASE_SAVE]: { status: JOB_STATUS.PENDING },
        [WORKFLOW_STAGES.SHOPIFY_SYNC]: { status: JOB_STATUS.PENDING },
        [WORKFLOW_STAGES.STATUS_UPDATE]: { status: JOB_STATUS.PENDING }
      },
      data: workflowData
    }
    
    await this.redisManager.set(`workflow:${workflowId}`, JSON.stringify(metadata), 86400) // 24 hour TTL
  }

  /**
   * Update workflow status
   */
  async updateWorkflowStatus(workflowId, status, additionalData = {}) {
    try {
      const metadataKey = `workflow:${workflowId}`
      const existing = await this.redisManager.get(metadataKey)
      
      if (existing) {
        const metadata = JSON.parse(existing)
        metadata.status = status
        metadata.updatedAt = new Date().toISOString()
        
        Object.assign(metadata, additionalData)
        
        await this.redisManager.set(metadataKey, JSON.stringify(metadata), 86400)
      }
    } catch (error) {
      console.warn(`‚ö†Ô∏è Failed to update workflow status for ${workflowId} - ${error.message}`);
      // Don't throw - allow job processing to continue even if status update fails
    }
  }

  /**
   * Update job metadata for specific stage
   */
  async updateJobMetadata(workflowId, stage, status, additionalData = {}) {
    try {
      const metadataKey = `workflow:${workflowId}`
      const existing = await this.redisManager.get(metadataKey)
      
      if (existing) {
        const metadata = JSON.parse(existing)
        
        if (!metadata.stages[stage]) {
          metadata.stages[stage] = {}
        }
        
        metadata.stages[stage].status = status
        metadata.stages[stage].updatedAt = new Date().toISOString()
        
        Object.assign(metadata.stages[stage], additionalData)
        
        await this.redisManager.set(metadataKey, JSON.stringify(metadata), 86400)
      }
    } catch (error) {
      console.warn(`‚ö†Ô∏è Failed to update job metadata for ${workflowId}:${stage} - ${error.message}`);
      // Don't throw - allow job processing to continue even if metadata update fails
    }
  }

  /**
   * Get workflow metadata
   */
  async getWorkflowMetadata(workflowId, key = null) {
    const metadataKey = `workflow:${workflowId}`
    const existing = await this.redisManager.get(metadataKey)
    
    if (!existing) return null
    
    const metadata = JSON.parse(existing)
    
    return key ? metadata.data[key] : metadata
  }

  /**
   * Get workflow status
   */
  async getWorkflowStatus(workflowId) {
    return await this.getWorkflowMetadata(workflowId)
  }

  /**
   * Get orchestrator statistics
   */
  getStatistics() {
    return {
      ...this.stats,
      queues: Array.from(this.queues.entries()).map(([type, queue]) => ({
        type,
        waiting: queue.waiting ? queue.waiting.length : 0,
        active: queue.active ? queue.active.length : 0,
        completed: queue.completed ? queue.completed.length : 0,
        failed: queue.failed ? queue.failed.length : 0
      }))
    }
  }

  /**
   * Cleanup completed workflows (housekeeping)
   */
  async cleanup(olderThanHours = 24) {
    const cutoffTime = new Date(Date.now() - (olderThanHours * 60 * 60 * 1000))
    
    // This would require scanning Redis keys and removing old workflows
    // Implementation depends on Redis key management strategy
    console.log(`üßπ Cleaning up workflows older than ${olderThanHours} hours`)
  }

  /**
   * Graceful shutdown
   */
  async shutdown() {
    console.log('üõë Shutting down Workflow Orchestrator...')
    
    const shutdownPromises = Array.from(this.queues.values()).map(queue => 
      queue.close()
    )
    
    await Promise.all(shutdownPromises)
    
    console.log('‚úÖ Workflow Orchestrator shutdown complete')
  }
}

// Create singleton instance
export const workflowOrchestrator = new WorkflowOrchestrator()